import matplotlib.pyplot as plt

# sgd_train = [0.3946494, 0.43655357, 0.45026305, 0.47289363, 0.49744567, 0.5199085, 0.5403889, 0.5636599,
#              0.59060615, 0.625467, 0.66441476, 0.71112466, 0.7570263, 0.80381244, 0.83762103, 0.86879146,
#              0.8980557, 0.91719407, 0.93212354, 0.94372857, 0.9533664, 0.9626687, 0.9672741, 0.9724285,
#              0.97744566, 0.9798246, 0.9814106, 0.9814411, 0.98357606, 0.9851315, 0.987907, 0.98722076,
#              0.9899504, 0.9891422, 0.99327487, 0.9927259, 0.99324435, 0.9940374, 0.9907434, 0.99522686,
#              0.9953946, 0.99402213, 0.99306136, 0.99350363, 0.99550134, 0.9985665, 0.9980785, 0.99458635,
#              0.99540985, 0.9965078]
#
# sgd_val = [0.41399336, 0.42146018, 0.45768806, 0.48230088, 0.5011062, 0.52571905, 0.5409292, 0.5528208,
#            0.5608407, 0.5575221, 0.57992256, 0.59457964, 0.5829646, 0.5862832, 0.585177, 0.60896015, 0.59457964,
#            0.6020465, 0.6125553, 0.60868365, 0.6285951, 0.6225111, 0.6214049, 0.63384956, 0.6498894, 0.6482301,
#            0.6465708, 0.6308075, 0.6490597, 0.647677, 0.6368916, 0.64961284, 0.642146, 0.6670354, 0.65486723,
#            0.6568031, 0.6706305, 0.6670354, 0.66288716, 0.6653761, 0.6642699, 0.6609513, 0.6598451, 0.64629424,
#            0.6664823, 0.67809737, 0.6816925, 0.65099555, 0.6601217, 0.66150445]
#
# adam_train = [0.42019817, 0.47725505, 0.5370797, 0.58542126, 0.63052994, 0.6718109, 0.7128936, 0.746382, 0.77903163,
#               0.8077468, 0.826809, 0.8472894, 0.86845595, 0.8783988, 0.8912238, 0.9042928, 0.91525733, 0.9220282,
#               0.9274114, 0.933679, 0.9421883, 0.94691575, 0.94982845, 0.9544796, 0.9562486, 0.95849025, 0.96440715,
#               0.96544415, 0.96669465, 0.96666414, 0.97130007, 0.9721998, 0.9728555, 0.97332823, 0.97645444, 0.97662216,
#               0.9773847, 0.9802974, 0.9808006, 0.9801144, 0.9824171, 0.9801601, 0.9821273, 0.985589, 0.9837743,
#               0.9834388, 0.98443, 0.98627526, 0.9861228, 0.98400307]
#
# adam_val = [0.44469026, 0.49751106, 0.54065263, 0.56913716, 0.5901549, 0.6073009, 0.616427, 0.60259956, 0.60536504,
#             0.6095133, 0.61117256, 0.6147677, 0.6120022, 0.60978985, 0.6167035, 0.59070796, 0.5887721, 0.5976217,
#             0.59292036, 0.61615044, 0.6081305, 0.60481197, 0.6178097, 0.6070243, 0.61725664, 0.62057525, 0.6175332,
#             0.61974555, 0.6056416, 0.5926438, 0.5915376, 0.60868365, 0.6114491, 0.6106195, 0.6056416, 0.59568584,
#             0.6186394, 0.6175332, 0.6186394, 0.60038716, 0.6136615, 0.60536504, 0.59928095, 0.5998341, 0.61283183,
#             0.5998341, 0.6045354, 0.60757744, 0.5948562, 0.6067478]
#
# rmsprop_train = [0.42082316, 0.5053145, 0.5457415, 0.57502097, 0.5968433, 0.6132367, 0.6281662, 0.63967973, 0.6522455,
#                  0.66179186, 0.6721769, 0.6785208, 0.6904308, 0.68820435, 0.6926725, 0.69778115, 0.71734655, 0.7174838,
#                  0.7171483, 0.72930235, 0.73704916, 0.7465345, 0.7514297, 0.75452536, 0.7626839, 0.7710408, 0.76889056,
#                  0.7677926, 0.77467024, 0.77650017, 0.78836447, 0.7896149, 0.801281, 0.8030042, 0.8028822, 0.8091956,
#                  0.8112238, 0.8126268, 0.82525355, 0.827724, 0.826138, 0.8328479, 0.8357606, 0.83762103, 0.83983225,
#                  0.8445902, 0.84469694, 0.84295845, 0.8397865, 0.8471521]
#
# rmsprop_val = [0.45298672, 0.54728985, 0.5658186, 0.57190263, 0.5807522, 0.5931969, 0.5931969, 0.576604, 0.5757743,
#                0.5948562, 0.57411504, 0.59292036, 0.5813053, 0.5577987, 0.58102876, 0.5909845, 0.5835177, 0.5824115,
#                0.59292036, 0.6020465, 0.60536504, 0.58600664, 0.57356197, 0.55724555, 0.5445243, 0.60591817, 0.5948562,
#                0.58711284, 0.60425884, 0.5995575, 0.5790929, 0.6020465, 0.5782633, 0.5887721, 0.59928095, 0.5707965,
#                0.55033183, 0.54397124, 0.6017699, 0.59679204, 0.5876659, 0.5738385, 0.57466817, 0.5807522, 0.6009403,
#                0.5954093, 0.5890487, 0.58268803, 0.58821905, 0.46321902]
#
# plt.plot(sgd_train, label='SGD_train')
# plt.plot(sgd_val, label='SGD_val')
# plt.plot(rmsprop_train, label='RMSprop_train', marker='x')
# plt.plot(rmsprop_val, label='RMSprop_val', marker='x')
# plt.plot(adam_train, label='Adam_train', marker='.')
# plt.plot(adam_val, label='Adam_val', marker='.')
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.4, 1])
# plt.legend(loc='upper left')

# plt.grid(True)
# plt.show()

# sgd_train = [0.66243064, 0.70840824, 0.7165483, 0.7211232, 0.7239312, 0.72787505, 0.7317558, 0.7354157, 0.73913866,
#              0.7425146, 0.7480675, 0.751191, 0.7545985, 0.75888944, 0.76305413, 0.7674712, 0.77412844, 0.7801546,
#              0.7863701, 0.7928695, 0.8016091, 0.8079192, 0.81706893, 0.82653415, 0.8397539, 0.8504496, 0.86297524,
#              0.8737656, 0.89023507, 0.8956933, 0.90487456, 0.91304624, 0.92153335, 0.9285376, 0.93377507, 0.9403691,
#              0.9481622, 0.9499606, 0.95437765, 0.95942575, 0.96179205, 0.9648525, 0.97087866, 0.9702792, 0.97229844,
#              0.97397065, 0.9758952, 0.97889256, 0.97778827, 0.9785455]
#
# sgd_val = [0.6897727, 0.6670455, 0.65170455, 0.6715909, 0.7130682, 0.6931818, 0.7096591, 0.71079546, 0.72386366,
#            0.71875, 0.7335227, 0.7278409, 0.7318182, 0.72329545, 0.73295456, 0.72897726, 0.7443182, 0.7443182,
#            0.7039773, 0.74715906, 0.74488634, 0.7426136, 0.72670454, 0.73977274, 0.7335227, 0.7596591, 0.7465909,
#            0.75284094, 0.75, 0.7227273, 0.74375, 0.7125, 0.71363634, 0.7403409, 0.72897726, 0.76193184, 0.7568182,
#            0.7647727, 0.725, 0.76931816, 0.7534091, 0.76193184, 0.7596591, 0.77102274, 0.77670455, 0.7568182, 0.7704545,
#            0.74147725, 0.7721591, 0.7477273]
#
# adam_train = [0.6757379, 0.7149708, 0.7236157, 0.7392018, 0.7513803, 0.7604039, 0.7694589, 0.7798706, 0.7930273,
#               0.7996214, 0.81179994, 0.8225272, 0.83754534, 0.850071, 0.8592838, 0.87480676, 0.8834201, 0.89402115,
#               0.89976335, 0.9038965, 0.9131409, 0.9200189, 0.9285692, 0.9263922, 0.93320715, 0.9370563, 0.93664616,
#               0.9422622, 0.945228, 0.94806755, 0.9513804, 0.9535574, 0.9565231, 0.9546301, 0.95854235, 0.9599621,
#               0.9589841, 0.9632434, 0.9685755, 0.96633536, 0.96598834, 0.96655625, 0.9700899, 0.97289795, 0.97359204,
#               0.9740022, 0.9750118, 0.9757375, 0.9783878, 0.9778829]
#
# adam_val = [0.64602274, 0.66988635, 0.6943182, 0.7403409, 0.7477273, 0.7375, 0.74886364, 0.7607955, 0.76193184,
#             0.7653409, 0.7653409, 0.75795454, 0.75454545, 0.7403409, 0.7318182, 0.7443182, 0.76363635, 0.7556818,
#             0.7477273, 0.74545455, 0.7318182, 0.74375, 0.73238635, 0.75454545, 0.75852275, 0.7517046, 0.75795454,
#             0.74147725, 0.7409091, 0.74715906, 0.7431818, 0.74147725, 0.7465909, 0.73636365, 0.76022726, 0.73238635,
#             0.75454545, 0.7607955, 0.74488634, 0.7613636, 0.7465909, 0.72329545, 0.74375, 0.74545455, 0.74147725,
#             0.7443182, 0.74886364, 0.75454545, 0.74147725, 0.7261364]
#
# rmsprop_train = [0.66952574, 0.71411896, 0.72711784, 0.7379397, 0.74421835, 0.75292635, 0.7569333, 0.7622653, 0.766714,
#                  0.7669664, 0.764253, 0.7657359, 0.76393753, 0.7660514, 0.7698375, 0.7663985, 0.7676921, 0.76917493,
#                  0.76835465, 0.7737498, 0.76974285, 0.7708156, 0.7685755, 0.7705632, 0.769522, 0.7733081, 0.7702792,
#                  0.77466476, 0.7753273, 0.7780092, 0.78192145, 0.77627385, 0.7718568, 0.7722354, 0.77393913, 0.77229846,
#                  0.7749487, 0.77141505, 0.77340275, 0.760183, 0.77214074, 0.7635274, 0.7578798, 0.75431454, 0.76141346,
#                  0.7615081, 0.7613819, 0.75797445, 0.7663985, 0.7423253]
#
# rmsprop_val = [0.6988636, 0.6943182, 0.69261366, 0.71079546, 0.7477273, 0.7125, 0.7255682, 0.7255682, 0.62954545,
#                0.7125, 0.7255682, 0.7346591, 0.73977274, 0.73125, 0.71761364, 0.7153409, 0.73977274, 0.72897726,
#                0.6659091, 0.7539773, 0.70454544, 0.74147725, 0.7056818, 0.68863636, 0.7096591, 0.74488634, 0.71988636,
#                0.7261364, 0.7164773, 0.7130682, 0.7386364, 0.69943184, 0.67897725, 0.7318182, 0.71875, 0.7426136,
#                0.7255682, 0.6, 0.58181816, 0.6653409, 0.74375, 0.5647727, 0.7443182, 0.75113636, 0.71420455, 0.6693182,
#                0.66136366, 0.71079546, 0.66136366, 0.6982955]
#
# plt.plot(sgd_train, label='SGD_train', color='blue')
# plt.plot(sgd_val, label='SGD_val', color='blue', marker='^')
# plt.plot(rmsprop_train, label='RMSprop_train', color='red')
# plt.plot(rmsprop_val, label='RMSprop_val', color='red', marker='x')
# plt.plot(adam_train, label='Adam_train', color='green')
# plt.plot(adam_val, label='Adam_val', color='green', marker='.')
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.5, 1])
# plt.legend(loc='lower center')
#
# plt.grid(True)
# plt.show()
#
# sgd_train = [0.795274, 0.8241012, 0.8420456, 0.8538353, 0.8626099, 0.8703034, 0.87708193, 0.8838812, 0.8905142,
#              0.89825964, 0.9034267, 0.9148317, 0.9241574, 0.9360198, 0.9384526, 0.95457757, 0.9573327, 0.9677396,
#              0.9708586, 0.9775539, 0.98514336, 0.9868172, 0.98819995, 0.9885534, 0.99288875, 0.9931487, 0.99137086,
#              0.9948849, 0.9964756, 0.9973073, 0.9973073, 0.9965795, 0.9967875, 0.99795187, 0.9987628, 0.9976504,
#              0.9980974, 0.99797267, 0.99779594, 0.9972345, 0.9963924, 0.99772316, 0.9987108, 0.9982118, 0.9994802,
#              0.9998856, 0.9999376, 0.9995945, 0.9999896, 1.0]
#
# sgd_val = [0.81006736, 0.7995883, 0.8504865, 0.8604042, 0.8654566, 0.86938626, 0.8727545, 0.8800524, 0.8836078,
#            0.8875374, 0.8819237, 0.8733159, 0.8729416, 0.8851048, 0.8699476, 0.8789296, 0.8727545, 0.8701347, 0.8785554,
#            0.86470807, 0.869012, 0.87462574, 0.87743264, 0.8849177, 0.87649703, 0.88042665, 0.8819237, 0.8895958,
#            0.88922155, 0.8845434, 0.8867889, 0.88529193, 0.8867889, 0.8866018, 0.88697606, 0.8877246, 0.8894087,
#            0.8845434, 0.8907186, 0.8817365, 0.8880988, 0.8938997, 0.8817365, 0.8877246, 0.89296407, 0.89483535,
#            0.89502245, 0.8953967, 0.8952096, 0.89502245]
#
# adam_train = [0.8220746, 0.8621109, 0.88106376, 0.89548373, 0.907315, 0.9201235, 0.9284927, 0.938411, 0.94740397,
#               0.9551078, 0.9602749, 0.96575385, 0.96971494, 0.9736344, 0.97794896, 0.9785, 0.98136944, 0.9828977,
#               0.98419726, 0.9864221, 0.98789847, 0.98883414, 0.9894787, 0.99035203, 0.99000895, 0.99116296, 0.9916932,
#               0.99216104, 0.9927848, 0.9938245, 0.99360615, 0.99414676, 0.9937621, 0.99446905, 0.9949369, 0.9945626,
#               0.9939388, 0.9960597, 0.9951552, 0.99506164, 0.9960909, 0.9964444, 0.9962053, 0.99587256, 0.9963716,
#               0.9962261, 0.99627805, 0.996434, 0.9966523, 0.99664193]
#
# adam_val = [0.8536677, 0.8787425, 0.89127994, 0.8916542, 0.88267213, 0.8856662, 0.88398206, 0.8895958, 0.89801645,
#             0.8847305, 0.8982036, 0.89202845, 0.8895958, 0.88342065, 0.8935254, 0.8903443, 0.8905314, 0.89502245,
#             0.8909057, 0.8997006, 0.89408684, 0.8944611, 0.8918413, 0.8955838, 0.8924027, 0.89277697, 0.8903443,
#             0.88342065, 0.8895958, 0.88641465, 0.89202845, 0.88622755, 0.8897829, 0.88416916, 0.8856662, 0.8879117,
#             0.8894087, 0.8905314, 0.8905314, 0.89876497, 0.88529193, 0.89408684, 0.88622755, 0.88922155, 0.8982036,
#             0.8888473, 0.8938997, 0.8895958, 0.8894087, 0.8909057]
#
# rmsprop_train = [0.8153173, 0.8568919, 0.8658225, 0.86807853, 0.8690766, 0.8738798, 0.87205, 0.8705737, 0.8723099,
#                  0.86855674, 0.86563534, 0.85732853, 0.85921025, 0.8579419, 0.85299313, 0.85303473, 0.854064,
#                  0.83840686, 0.8290084, 0.81674045, 0.7963633, 0.7868609, 0.8274281, 0.8310669, 0.76810557, 0.73640656,
#                  0.81834155, 0.75967395, 0.7639989, 0.7442455, 0.5938494, 0.5879546, 0.5962094, 0.6103799, 0.6127711,
#                  0.6019587, 0.6136132, 0.60724014, 0.6077911, 0.616098, 0.6130726, 0.60362214, 0.6077496, 0.60379887,
#                  0.61351967, 0.60810304, 0.60418355, 0.6131973, 0.60658514, 0.60515046]
#
# rmsprop_val = [0.83177394, 0.8358907, 0.88117516, 0.87238026, 0.86583084, 0.877994, 0.88622755, 0.8633982, 0.8721931,
#                0.8701347, 0.86938626, 0.8488024, 0.8409431, 0.88323355, 0.88061374, 0.8748129, 0.8205464, 0.7297904,
#                0.619012, 0.8059506, 0.85797155, 0.8787425, 0.85872006, 0.87256736, 0.5097305, 0.61676645, 0.82466316,
#                0.67234284, 0.59711826, 0.5301272, 0.502994, 0.5116018, 0.54247755, 0.5770958, 0.6085329, 0.5479042,
#                0.528256, 0.54752994, 0.5778443, 0.559506, 0.55707335, 0.5692365, 0.53574103, 0.5662425, 0.5604416,
#                0.5668039, 0.5623129, 0.5660554, 0.56212574, 0.5589446]
#
# plt.plot(sgd_train, label='SGD_train')
# plt.plot(sgd_val, label='SGD_val')
# plt.plot(rmsprop_train, label='RMSprop_train', marker='x')
# plt.plot(rmsprop_val, label='RMSprop_val', marker='x')
# plt.plot(adam_train, label='Adam_train', marker='.')
# plt.plot(adam_val, label='Adam_val', marker='.')
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.5, 1])
# plt.legend(loc='right')
#
# plt.grid(True)
# plt.show()
#
# """ BINARY NEG/POSITIVE"""
# l1l1_train = [0.7865935, 0.8201401, 0.84640175, 0.8569958, 0.8629946, 0.8688167, 0.87364066, 0.8768012, 0.88025284,
#               0.8851288, 0.88860124, 0.8929158, 0.8967521, 0.89935124, 0.9034787, 0.90647286, 0.9090928, 0.9117127,
#               0.91345936, 0.9164639, 0.91791946, 0.920425, 0.92272264, 0.92328405, 0.9258312, 0.92749465, 0.92812884,
#               0.9292932, 0.9303745, 0.9321419, 0.93224585, 0.93281764, 0.93549997, 0.93557274]
#
# l1l1_val = [0.7883608, 0.83345807, 0.85591316, 0.84337574, 0.85591316, 0.8574102, 0.8596557, 0.8669536, 0.8766841,
#             0.8880988, 0.8924027, 0.89109284, 0.88716316, 0.89502245, 0.900262, 0.8952096, 0.8909057, 0.8808009,
#             0.9043788, 0.9083084, 0.9133608, 0.9171033, 0.91148955, 0.91242516, 0.912238, 0.91261226, 0.9206587,
#             0.9247754, 0.9249626, 0.9084955, 0.91186374, 0.9133608, 0.9148578, 0.91766465]
#
# l1l2_train = [0.7939538, 0.8344873, 0.8508723, 0.8615391, 0.8720292, 0.878475, 0.8848065, 0.8902231, 0.8944233,
#               0.8980517, 0.90045327, 0.9033331, 0.90493417, 0.9070239, 0.9081883, 0.91033, 0.910985, 0.9136049,
#               0.91541386]
#
# l1l2_val = [0.8085704, 0.83757484, 0.8585329, 0.8439371, 0.85310626, 0.8555389, 0.8450599, 0.8699476, 0.89502245,
#             0.903256, 0.8993263, 0.89782935, 0.9034431, 0.9079341, 0.9055015, 0.8880988, 0.89726794, 0.8856662,
#             0.8858533]
#
# l2l1_train = [0.7995675, 0.8281247, 0.85234857, 0.86379516, 0.8723827, 0.88167715, 0.88855964, 0.8947144, 0.8994032,
#               0.9026158, 0.9058595, 0.90834427, 0.91135925, 0.91366726, 0.91541386, 0.91843927, 0.91982204, 0.9229098,
#               0.924043, 0.9255609]
#
# l2l1_val = [0.81886226, 0.8340195, 0.8553518, 0.8632111, 0.869012, 0.87163174, 0.8931512, 0.8976422, 0.9051272,
#             0.9090569, 0.9043788, 0.9013847, 0.90886974, 0.91092813, 0.91672903, 0.90681136, 0.8976422, 0.89127994,
#             0.89577097, 0.8974551]
#
# l2l2_train = [0.80228084, 0.8396752, 0.8605722, 0.8683592, 0.87404615, 0.8796914, 0.88469213, 0.89008796, 0.8930718,
#               0.8962323, 0.8971264, 0.90056765, 0.9010043, 0.9045911, 0.9055268, 0.9077101, 0.909176]
#
# l2l2_val = [0.8212949, 0.8592814, 0.8671407, 0.86938626, 0.8759356, 0.8819237, 0.8877246, 0.89408684, 0.89408684,
#             0.89502245, 0.89876497, 0.900262, 0.8997006, 0.8894087, 0.88323355, 0.8738772, 0.88529193]
#
# plt.plot(l1l1_train, label='L1/L1 - Train')
# plt.plot(l1l1_val, label='L1/L1 - Val')
#
# plt.plot(l1l2_train, label='L1/L2 - Train', marker='x')
# plt.plot(l1l2_val, label='L1/L2 - Val', marker='x')
#
# plt.plot(l2l1_train, label='L2/L1 - Train', marker='.')
# plt.plot(l2l1_val, label='L2/L1 - Val', marker='.')
#
# plt.plot(l2l2_train, label='L2/L2 - Train', marker='^')
# plt.plot(l2l2_val, label='L2/L2 - Val', marker='^')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.0, 1])
# plt.legend(loc='lower right')
#
# plt.grid(True)
# plt.show()
#
# """ FIVE MODEL"""
# l1l1_train = [0.23118903, 0.23020968, 0.23016393, 0.23016393, 0.23019443, 0.23020968]
#
# l1l1_val = [0.23810841, 0.23810841, 0.23810841, 0.23810841, 0.23810841, 0.23810841]
#
# l1l2_train = [0.23118903, 0.2376668, 0.35676706, 0.39900878, 0.412337, 0.4161037, 0.41718644, 0.4201449, 0.42087686,
#               0.42139536, 0.4225391, 0.42400306, 0.4232253]
#
# l1l2_val = [0.23810841, 0.29065266, 0.3863385, 0.39076328, 0.4045907, 0.41454646, 0.42035398, 0.43003318, 0.41980088,
#             0.41731194, 0.4256084, 0.39878318, 0.42118362]
#
# l2l1_train = [0.36727133, 0.40954632, 0.4183454, 0.4238963, 0.42841023, 0.4314754, 0.43231416, 0.43434235, 0.4353946,
#               0.4372398, 0.43861228, 0.4423942, 0.4455814, 0.4501258, 0.45463973, 0.4585284, 0.46500954, 0.47155166,
#               0.4774533, 0.4820587, 0.4873656, 0.4913458, 0.49593595, 0.5018986, 0.50555855, 0.51086545, 0.5176668,
#               0.52448344, 0.5289363, 0.5336485, 0.5373542, 0.54263055, 0.54963017, 0.55437285, 0.55902404, 0.5649257,
#               0.5701411, 0.5756767, 0.5804956, 0.5836676, 0.5883645, 0.59086543]
#
# l2l1_val = [0.41178098, 0.43086284, 0.4186947, 0.42090708, 0.42948008, 0.43390486, 0.43943584, 0.44054204, 0.4256084,
#             0.41261062, 0.4159292, 0.43888274, 0.446073, 0.44330752, 0.42311946, 0.45436946, 0.4579646, 0.44386062,
#             0.48561946, 0.47759956, 0.49668142, 0.50027657, 0.49004424, 0.5058075, 0.49917036, 0.5124447, 0.5058075,
#             0.5154867, 0.5301438, 0.53539824, 0.49115044, 0.5442478, 0.5262721, 0.5373341, 0.5425885, 0.545354,
#             0.56443584, 0.5577987, 0.55835176, 0.5420354, 0.5470133, 0.5334624]
#
# l2l2_train = [0.3935823, 0.4311704, 0.43756005, 0.44420892, 0.452764, 0.47080442, 0.4885551, 0.5036828, 0.5186733,
#               0.533618, 0.5474342, 0.5640107, 0.58185285, 0.5998475, 0.6184217, 0.63670605, 0.6580862, 0.68059474,
#               0.6985741, 0.72204345, 0.7436371, 0.7634769, 0.7777964, 0.7951811, 0.8019367, 0.81808615, 0.8276172,
#               0.8377125, 0.84474266, 0.85765916, 0.86061764, 0.86743426, 0.8737324, 0.87740755, 0.8828822, 0.88471216,
#               0.8904765, 0.89268774, 0.89755243, 0.8960122, 0.9033931]
#
# l2l2_val = [0.43362832, 0.43611726, 0.44137168, 0.44551992, 0.460177, 0.48727876, 0.5091261, 0.5199115, 0.51963496,
#             0.5240597, 0.5470133, 0.53152657, 0.545354, 0.5608407, 0.5566925, 0.5608407, 0.55835176, 0.5370575,
#             0.5660951, 0.5699668, 0.553927, 0.5630531, 0.5608407, 0.5685841, 0.5801991, 0.5674779, 0.5749447, 0.5909845,
#             0.58379424, 0.5813053, 0.59679204, 0.5959624, 0.58268803, 0.58600664, 0.58490044, 0.6061947, 0.5948562,
#             0.5959624, 0.59402657, 0.5818584, 0.5915376]
#
# plt.plot(l1l1_train, label='L1/L1 - Train')
# plt.plot(l1l1_val, label='L1/L1 - Val')
#
# plt.plot(l1l2_train, label='L1/L2 - Train', marker='x')
# plt.plot(l1l2_val, label='L1/L2 - Val', marker='x')
#
# plt.plot(l2l1_train, label='L2/L1 - Train', marker='.')
# plt.plot(l2l1_val, label='L2/L1 - Val', marker='.')
#
# plt.plot(l2l2_train, label='L2/L2 - Train', marker='^')
# plt.plot(l2l2_val, label='L2/L2 - Val', marker='^')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.1, 1])
# plt.legend(loc='lower right')
#
# plt.grid(True)
# plt.show()
#
# """ MALIGNANT MASS REGUL """
# l1l1_train = [0.5984485, 0.5984856, 0.5986749, 0.5984856, 0.5984856, 0.5985171]
#
# l1l1_val = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]
#
# l1l2_train = [0.59848005, 0.5984856, 0.5986749, 0.5984856, 0.59858024, 0.5985171]
#
# l1l2_val = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]
#
# l2l1_train = [0.5975971, 0.67114687, 0.70086765, 0.70553714, 0.7066099, 0.7071778, 0.7081874]
#
# l2l1_val = [0.5, 0.70681816, 0.69545454, 0.6982955, 0.6982955, 0.67329544, 0.66875]
#
# l2l2_train = [0.66003406, 0.7015302, 0.705127, 0.70575804, 0.7055056, 0.708219, 0.7108692, 0.7143398, 0.7189462,
#               0.7221013, 0.7236157, 0.725351, 0.7268654, 0.72459376, 0.73096704, 0.7343114, 0.736867, 0.7422622,
#               0.74945575, 0.7560499, 0.7597413, 0.76251775, 0.76359046, 0.76560974]
#
# l2l2_val = [0.7056818, 0.6454545, 0.6619318, 0.67045456, 0.70625, 0.6721591, 0.70454544, 0.6943182, 0.7193182,
#             0.7284091, 0.7346591, 0.7340909, 0.7386364, 0.7460227, 0.7181818, 0.74147725, 0.7409091, 0.7346591,
#             0.7517046, 0.74886364, 0.74545455, 0.7221591, 0.7301136, 0.70511365]
#
# plt.plot(l1l1_train, label='L1/L1 - Train')
# plt.plot(l1l1_val, label='L1/L1 - Val')
#
# plt.plot(l1l2_train, label='L1/L2 - Train', marker='x')
# plt.plot(l1l2_val, label='L1/L2 - Val', marker='x')
#
# plt.plot(l2l1_train, label='L2/L1 - Train', marker='.')
# plt.plot(l2l1_val, label='L2/L1 - Val', marker='.')
#
# plt.plot(l2l2_train, label='L2/L2 - Train', marker='^')
# plt.plot(l2l2_val, label='L2/L2 - Val', marker='^')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.45, 0.8])
# plt.legend(loc='lower right')
#
# plt.grid(True)
# plt.show()
#
# """MM MIN MAX"""
# max_pool_tr = [0.6757379, 0.7149708, 0.7236157, 0.7392018, 0.7513803, 0.7604039, 0.7694589, 0.7798706, 0.7930273,
#                0.7996214, 0.81179994, 0.8225272, 0.83754534, 0.850071, 0.8592838, 0.87480676, 0.8834201, 0.89402115,
#                0.89976335, 0.9038965, 0.9131409, 0.9200189, 0.9285692, 0.9263922, 0.93320715, 0.9370563, 0.93664616,
#                0.9422622, 0.945228, 0.94806755, 0.9513804, 0.9535574, 0.9565231, 0.9546301, 0.95854235, 0.9599621,
#                0.9589841, 0.9632434, 0.9685755, 0.96633536, 0.96598834, 0.96655625, 0.9700899, 0.97289795, 0.97359204,
#                0.9740022, 0.9750118, 0.9757375, 0.9783878, 0.9778829]
#
# max_pool_val = [0.64602274, 0.66988635, 0.6943182, 0.7403409, 0.7477273, 0.7375, 0.74886364, 0.7607955, 0.76193184,
#                 0.7653409, 0.7653409, 0.75795454, 0.75454545, 0.7403409, 0.7318182, 0.7443182, 0.76363635, 0.7556818,
#                 0.7477273, 0.74545455, 0.7318182, 0.74375, 0.73238635, 0.75454545, 0.75852275, 0.7517046, 0.75795454,
#                 0.74147725, 0.7409091, 0.74715906, 0.7431818, 0.74147725, 0.7465909, 0.73636365, 0.76022726, 0.73238635,
#                 0.75454545, 0.7607955, 0.74488634, 0.7613636, 0.7465909, 0.72329545, 0.74375, 0.74545455, 0.74147725,
#                 0.7443182, 0.74886364, 0.75454545, 0.74147725, 0.7261364]
#
# min_pool_tr = [0.67904896, 0.7168639, 0.7250986, 0.72913706, 0.73626757, 0.7404007, 0.7460798, 0.752106, 0.7571857,
#                0.7634958, 0.7722669, 0.7818899, 0.7860546, 0.79662406, 0.8059315, 0.8086764, 0.81769997, 0.82366306,
#                0.8318662, 0.83760846, 0.8432876, 0.8503865, 0.8570753, 0.8674239, 0.86938006, 0.8741442, 0.88196874,
#                0.8907714, 0.8964821, 0.9033286, 0.91045904, 0.9164537, 0.9208077, 0.9296104, 0.93317556, 0.9352895,
#                0.93390125, 0.94204134, 0.94664776, 0.94620603, 0.9485092, 0.95642847, 0.9552295, 0.9581322, 0.95797443,
#                0.9610033, 0.9616974, 0.9626755, 0.9646316, 0.9653258]
#
# min_pool_val = [0.68125,
#                 0.6494318,
#                 0.6721591,
#                 0.70681816,
#                 0.72727275,
#                 0.7403409,
#                 0.7403409,
#                 0.7426136,
#                 0.74715906,
#                 0.7460227,
#                 0.7517046,
#                 0.75625,
#                 0.75511366,
#                 0.7534091,
#                 0.76761365,
#                 0.7522727,
#                 0.7539773,
#                 0.74204546,
#                 0.7539773,
#                 0.7278409,
#                 0.7284091,
#                 0.70852274,
#                 0.7153409,
#                 0.7073864,
#                 0.70625,
#                 0.7221591,
#                 0.7181818,
#                 0.72386366,
#                 0.72329545,
#                 0.7431818,
#                 0.74488634,
#                 0.74147725,
#                 0.7426136,
#                 0.76193184,
#                 0.76022726,
#                 0.75454545,
#                 0.7647727,
#                 0.75511366,
#                 0.7392045,
#                 0.75454545,
#                 0.77272725,
#                 0.77613634,
#                 0.7698864,
#                 0.7613636,
#                 0.74886364,
#                 0.7715909,
#                 0.7625,
#                 0.74545455,
#                 0.7613636,
#                 0.73579544]
#
# avg_pool_train = [0.6701249, 0.7146553, 0.7209023, 0.73150337, 0.73986435, 0.7457643, 0.7288531, 0.7236157, 0.739044,
#                   0.7141505, 0.7140243, 0.7317873, 0.74418676, 0.75434613, 0.7684177, 0.7829626, 0.79182833, 0.801546,
#                   0.8077299, 0.8113583, 0.8213283, 0.82908976, 0.83669347, 0.84101593, 0.84669507, 0.84754694,
#                   0.8558448, 0.85950464, 0.8698533, 0.870926, 0.8728822, 0.87717307, 0.8799495, 0.8853762, 0.8857548,
#                   0.8915602, 0.8966083, 0.89670295, 0.89963716, 0.90471685, 0.90613663, 0.907935, 0.91282535,
#                   0.91503394, 0.9137088, 0.91591734, 0.9216596, 0.91885155, 0.9185045, 0.92812747]
# aver_pool_val = [0.6869318, 0.6857954, 0.70113635, 0.7090909, 0.73125, 0.7346591, 0.73068184, 0.7340909, 0.7426136,
#                  0.675, 0.71022725, 0.73579544, 0.7431818, 0.7409091, 0.75113636, 0.74488634, 0.73238635, 0.7778409,
#                  0.7443182, 0.76193184, 0.7522727, 0.76704544, 0.75625, 0.7659091, 0.7738636, 0.7772727, 0.7880682,
#                  0.775, 0.775, 0.7744318, 0.78409094, 0.76704544, 0.7704545, 0.7647727, 0.76931816, 0.76931816, 0.7875,
#                  0.7755682, 0.76420456, 0.7704545, 0.7625, 0.77329546, 0.7744318, 0.76420456, 0.77102274, 0.7659091,
#                  0.7522727, 0.7664773, 0.7517046, 0.7613636]
#
# plt.plot(max_pool_tr,
#          label='Max Pooling - Train')
# plt.plot(max_pool_val,
#          label='Max Pooling - Val')
#
# plt.plot(min_pool_tr,
#          label='Min Pooling - Train',
#          marker='x')
# plt.plot(min_pool_val,
#          label='Min Pooling - Val',
#          marker='x')
#
# plt.plot(avg_pool_train,
#          label='Avg Pooling - Train',
#          marker='.')
# plt.plot(aver_pool_val,
#          label='Avg Pooling - Val',
#          marker='.')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.6, 1])
# plt.legend(loc='upper left')
#
# plt.grid(True)
# plt.show()
#
# """ FIVE MIN MAZ"""
# max_pool_tr = [0.3946494, 0.43655357, 0.45026305, 0.47289363, 0.49744567, 0.5199085, 0.5403889, 0.5636599, 0.59060615,
#                0.625467, 0.66441476, 0.71112466, 0.7570263, 0.80381244, 0.83762103, 0.86879146, 0.8980557, 0.91719407,
#                0.93212354, 0.94372857, 0.9533664, 0.9626687, 0.9672741, 0.9724285, 0.97744566, 0.9798246, 0.9814106,
#                0.9814411, 0.98357606, 0.9851315, 0.987907, 0.98722076, 0.9899504, 0.9891422, 0.99327487, 0.9927259,
#                0.99324435, 0.9940374, 0.9907434, 0.99522686, 0.9953946, 0.99402213, 0.99306136, 0.99350363, 0.99550134,
#                0.9985665, 0.9980785, 0.99458635, 0.99540985, 0.9965078]
#
# max_pool_val = [0.41399336, 0.42146018, 0.45768806, 0.48230088, 0.5011062, 0.52571905, 0.5409292, 0.5528208, 0.5608407,
#                 0.5575221, 0.57992256, 0.59457964, 0.5829646, 0.5862832, 0.585177, 0.60896015, 0.59457964, 0.6020465,
#                 0.6125553, 0.60868365, 0.6285951, 0.6225111, 0.6214049, 0.63384956, 0.6498894, 0.6482301, 0.6465708,
#                 0.6308075, 0.6490597, 0.647677, 0.6368916, 0.64961284, 0.642146, 0.6670354, 0.65486723, 0.6568031,
#                 0.6706305, 0.6670354, 0.66288716, 0.6653761, 0.6642699, 0.6609513, 0.6598451, 0.64629424, 0.6664823,
#                 0.67809737, 0.6816925, 0.65099555, 0.6601217, 0.66150445]
#
# min_pool_tr = [0.3933689, 0.44304994, 0.454289, 0.46412504, 0.4770263, 0.4861304, 0.4981319, 0.5104537, 0.52405643,
#                0.5395044, 0.5557301, 0.5782539, 0.6057644, 0.63926804, 0.6793595, 0.72643536, 0.773008, 0.81313,
#                0.8484331, 0.8764316, 0.8962257, 0.9110027, 0.92698437, 0.93656117, 0.946016, 0.9503774, 0.9598475,
#                0.9622112, 0.9680976, 0.97252, 0.97553945, 0.97732365, 0.97823864, 0.9802974, 0.9818681, 0.98366755,
#                0.98261535, 0.98627526, 0.9892032, 0.99149066, 0.99127716, 0.99200916, 0.98927945, 0.9908197, 0.9929851,
#                0.9917347, 0.9945101, 0.9939611, 0.9939001, 0.9937781]
#
# min_pool_val = [0.4363938,
#                 0.44413716,
#                 0.45022124,
#                 0.4585177,
#                 0.47234514,
#                 0.47123894,
#                 0.47981194,
#                 0.49336284,
#                 0.49557522,
#                 0.46847346,
#                 0.4897677,
#                 0.5176991,
#                 0.5284845,
#                 0.52820796,
#                 0.52599555,
#                 0.54231197,
#                 0.545354,
#                 0.53650445,
#                 0.54977876,
#                 0.5478429,
#                 0.5577987,
#                 0.5597345,
#                 0.57632744,
#                 0.576604,
#                 0.57245576,
#                 0.5854535,
#                 0.5943031,
#                 0.60259956,
#                 0.6061947,
#                 0.5818584,
#                 0.5965155,
#                 0.5926438,
#                 0.6020465,
#                 0.6117257,
#                 0.5943031,
#                 0.60757744,
#                 0.60536504,
#                 0.60757744,
#                 0.6136615,
#                 0.6117257,
#                 0.6169801,
#                 0.6084071,
#                 0.5995575,
#                 0.6125553,
#                 0.6001106,
#                 0.62306416,
#                 0.6261062,
#                 0.6178097,
#                 0.6191925,
#                 0.61006635]
#
# avg_pool_train = [0.3957927, 0.4433092, 0.45369425, 0.46386582, 0.4745101, 0.48581013, 0.49706444, 0.51069766,
#                   0.52596265, 0.54275256, 0.56606936, 0.59274113, 0.62561953, 0.66179186, 0.70273733, 0.74679375,
#                   0.7812581, 0.8145787, 0.839573, 0.85860467, 0.88067096, 0.89523447, 0.90851694, 0.91852075,
#                   0.9294091, 0.9361342, 0.9425086, 0.95008767, 0.9548151, 0.9567518, 0.9626687, 0.9682959,
#                   0.96884483, 0.973435, 0.97457874, 0.9742127, 0.9784979, 0.9810751, 0.9819443, 0.9823561, 0.984735,
#                   0.9823561, 0.9852993, 0.987053, 0.9866565, 0.9889592, 0.9904384, 0.98982847, 0.9913534,
#                   0.9908654]
# aver_pool_val = [0.42920354, 0.44220132, 0.45602876, 0.46349558,
#                  0.47759956, 0.48589602, 0.4875553, 0.5033186,
#                  0.50884956, 0.49446902, 0.5016593, 0.5124447,
#                  0.5279314, 0.5154867, 0.5542035, 0.5348451,
#                  0.54618365, 0.5688606, 0.5674779, 0.5658186,
#                  0.55586284, 0.5915376, 0.5862832, 0.58600664,
#                  0.5984513, 0.602323, 0.60536504, 0.6095133,
#                  0.6200221, 0.6131084, 0.63274336, 0.6202987,
#                  0.62582964, 0.6200221, 0.6272124, 0.6330199,
#                  0.6136615, 0.6501659, 0.6186394, 0.62582964,
#                  0.6277655, 0.63550884, 0.6382743, 0.6313606,
#                  0.64684737, 0.6609513, 0.6498894, 0.6504425,
#                  0.6487832, 0.6493363]
#
# plt.plot(max_pool_tr,
#          label='Max Pooling - Train')
# plt.plot(max_pool_val,
#          label='Max Pooling - Val')
#
# plt.plot(min_pool_tr,
#          label='Min Pooling - Train',
#          marker='x')
# plt.plot(min_pool_val,
#          label='Min Pooling - Val',
#          marker='x')
#
# plt.plot(avg_pool_train,
#          label='Avg Pooling - Train',
#          marker='.')
# plt.plot(aver_pool_val,
#          label='Avg Pooling - Val',
#          marker='.')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.35, 1])
# plt.legend(loc='upper left')
#
# plt.grid(True)
# plt.show()

# """ NEGATIVE MIN MAX"""
#
# max_pool_tr = [0.8220746, 0.8621109, 0.88106376, 0.89548373, 0.907315, 0.9201235, 0.9284927, 0.938411, 0.94740397,
#                0.9551078, 0.9602749, 0.96575385, 0.96971494, 0.9736344, 0.97794896, 0.9785, 0.98136944, 0.9828977,
#                0.98419726, 0.9864221, 0.98789847, 0.98883414, 0.9894787, 0.99035203, 0.99000895, 0.99116296, 0.9916932,
#                0.99216104, 0.9927848, 0.9938245, 0.99360615, 0.99414676, 0.9937621, 0.99446905, 0.9949369, 0.9945626,
#                0.9939388, 0.9960597, 0.9951552, 0.99506164, 0.9960909, 0.9964444, 0.9962053, 0.99587256, 0.9963716,
#                0.9962261, 0.99627805, 0.996434, 0.9966523, 0.99664193]
#
# max_pool_val = [0.8536677, 0.8787425, 0.89127994, 0.8916542, 0.88267213, 0.8856662, 0.88398206, 0.8895958, 0.89801645,
#                 0.8847305, 0.8982036, 0.89202845, 0.8895958, 0.88342065, 0.8935254, 0.8903443, 0.8905314, 0.89502245,
#                 0.8909057, 0.8997006, 0.89408684, 0.8944611, 0.8918413, 0.8955838, 0.8924027, 0.89277697, 0.8903443,
#                 0.88342065, 0.8895958, 0.88641465, 0.89202845, 0.88622755, 0.8897829, 0.88416916, 0.8856662, 0.8879117,
#                 0.8894087, 0.8905314, 0.8905314, 0.89876497, 0.88529193, 0.89408684, 0.88622755, 0.88922155, 0.8982036,
#                 0.8888473, 0.8938997, 0.8895958, 0.8894087, 0.8909057]
#
# min_pool_tr = [0.81388265, 0.83655626, 0.84494627, 0.8498534, 0.8545318, 0.8587424, 0.8633897, 0.86765224, 0.8722787,
#                0.8775913, 0.8840164, 0.8882686, 0.8944025, 0.90314597, 0.9104028, 0.9186992, 0.9234608, 0.9285655,
#                0.93439794, 0.9374857, 0.94156116, 0.9481422, 0.95071006, 0.95442164, 0.9582372, 0.96088827, 0.9638721,
#                0.965785, 0.9694758, 0.9718774, 0.9726675, 0.973572, 0.975381, 0.9766806, 0.9791238, 0.97948766,
#                0.9808288, 0.98127586, 0.98163974, 0.9823883, 0.9839374, 0.9849978, 0.9856528, 0.9857879, 0.9867652,
#                0.9871499, 0.9881896, 0.98752415, 0.9882519, 0.9882935]
#
# min_pool_val = [0.80782187, 0.8401946, 0.8328967, 0.85104793, 0.84842813, 0.84636974, 0.8532934, 0.85198355, 0.85872006,
#                 0.8525449, 0.8564746, 0.8508608, 0.85572606, 0.8439371, 0.85198355, 0.8415045, 0.84992516, 0.84431136,
#                 0.8495509, 0.8446856, 0.8495509, 0.84842813, 0.8435629, 0.8450599, 0.85273206, 0.84225297, 0.8388847,
#                 0.8420659, 0.82073355, 0.8145584, 0.81699103, 0.8255988, 0.8310254, 0.8310254, 0.8400075, 0.83476794,
#                 0.8349551, 0.82859284, 0.837762, 0.840756, 0.83532935, 0.82297903, 0.83607787, 0.83233535, 0.8330838,
#                 0.83345807, 0.8255988, 0.84131736, 0.8476796, 0.83177394]
#
# avg_pool_train = [0.8138723, 0.8387395, 0.8451646, 0.86110246, 0.8669141, 0.87348473, 0.87879735, 0.88941216,
#                   0.90043247, 0.911536, 0.9216622, 0.93013537, 0.9398353, 0.94584453, 0.95102197, 0.95611626,
#                   0.96091944, 0.96642965, 0.9706922, 0.9730314, 0.974934, 0.9769925, 0.97915494, 0.9806313, 0.9829705,
#                   0.9829185, 0.98514336, 0.9852889, 0.9878465, 0.9880544, 0.98910445, 0.9889797, 0.98975945, 0.9907471,
#                   0.9908823, 0.9910694, 0.99231696, 0.9922234, 0.99214023, 0.9927848, 0.9931279, 0.9914229, 0.99258727,
#                   0.994022, 0.9943027, 0.99410516, 0.9943859, 0.99416757, 0.9952696, 0.9954983]
# aver_pool_val = [0.82092065, 0.8420659, 0.8495509, 0.84842813, 0.86152697, 0.86377245, 0.85778445, 0.8632111,
#                  0.86283684, 0.8652695, 0.8575973, 0.86152697, 0.8590943, 0.8575973, 0.8581587, 0.8566617, 0.85497755,
#                  0.86358535, 0.8602171, 0.8575973, 0.86077845, 0.85516465, 0.85591316, 0.85984284, 0.8594686, 0.8525449,
#                  0.8575973, 0.8574102, 0.8590943, 0.86096555, 0.86152697, 0.85778445, 0.8538548, 0.8575973, 0.86676645,
#                  0.85310626, 0.86077845, 0.85890716, 0.86283684, 0.86676645, 0.8592814, 0.8592814, 0.85516465,
#                  0.85497755, 0.8611527, 0.8561003, 0.8596557, 0.86302394, 0.8633982, 0.86096555]
#
# plt.plot(max_pool_tr,
#          label='Max Pooling - Train')
# plt.plot(max_pool_val,
#          label='Max Pooling - Val')
#
# plt.plot(min_pool_tr,
#          label='Min Pooling - Train',
#          marker='x')
# plt.plot(min_pool_val,
#          label='Min Pooling - Val',
#          marker='x')
#
# plt.plot(avg_pool_train,
#          label='Avg Pooling - Train',
#          marker='.')
# plt.plot(aver_pool_val,
#          label='Avg Pooling - Val',
#          marker='.')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.8, 1])
# plt.legend(loc='upper left')
#
# plt.grid(True)
# plt.show()
#
# batch32_tr = [0.3946494, 0.43655357, 0.45026305, 0.47289363, 0.49744567, 0.5199085, 0.5403889, 0.5636599, 0.59060615,
#               0.625467, 0.66441476, 0.71112466, 0.7570263, 0.80381244, 0.83762103, 0.86879146, 0.8980557, 0.91719407,
#               0.93212354, 0.94372857, 0.9533664, 0.9626687, 0.9672741, 0.9724285, 0.97744566, 0.9798246, 0.9814106,
#               0.9814411, 0.98357606, 0.9851315, 0.987907, 0.98722076, 0.9899504, 0.9891422, 0.99327487, 0.9927259,
#               0.99324435, 0.9940374, 0.9907434, 0.99522686, 0.9953946, 0.99402213, 0.99306136, 0.99350363, 0.99550134,
#               0.9985665, 0.9980785, 0.99458635, 0.99540985, 0.9965078]
#
# batch32_val = [0.41399336, 0.42146018, 0.45768806, 0.48230088, 0.5011062, 0.52571905, 0.5409292, 0.5528208, 0.5608407,
#                0.5575221, 0.57992256, 0.59457964, 0.5829646, 0.5862832, 0.585177, 0.60896015, 0.59457964, 0.6020465,
#                0.6125553, 0.60868365, 0.6285951, 0.6225111, 0.6214049, 0.63384956, 0.6498894, 0.6482301, 0.6465708,
#                0.6308075, 0.6490597, 0.647677, 0.6368916, 0.64961284, 0.642146, 0.6670354, 0.65486723, 0.6568031,
#                0.6706305, 0.6670354, 0.66288716, 0.6653761, 0.6642699, 0.6609513, 0.6598451, 0.64629424, 0.6664823,
#                0.67809737, 0.6816925, 0.65099555, 0.6601217, 0.66150445]
#
# batch64_tr = [0.38120428, 0.4305723, 0.44085562, 0.44733992, 0.45629588, 0.46961537, 0.48568115, 0.5026471, 0.5172177,
#               0.530644, 0.545001, 0.55980045, 0.57646126, 0.5946325, 0.61941016, 0.64630854, 0.6777535, 0.714447,
#               0.74886715, 0.78519446, 0.8212776, 0.84520084, 0.87302995, 0.8948934, 0.9137208, 0.9309156, 0.94139725,
#               0.9519552, 0.9567307, 0.96457285, 0.9705842, 0.97262865, 0.97650397, 0.97647345, 0.9790062, 0.9813252,
#               0.985582, 0.98159987, 0.9843004, 0.9903422, 0.990983, 0.9926155, 0.99568224, 0.99556017, 0.992585,
#               0.99427855, 0.9949041, 0.9947973, 0.99662817, 0.9984743]
#
# batch64_val = [0.41796875, 0.4188058, 0.41768974, 0.4486607, 0.46763393, 0.4718192, 0.50251114, 0.50334823, 0.5223214,
#                0.53487724, 0.5401786, 0.5298549, 0.5479911, 0.5410156, 0.547712, 0.53348213, 0.57477677, 0.55887276,
#                0.515625, 0.5485491, 0.578683, 0.5694755, 0.49776787, 0.58203125, 0.610212, 0.5876116, 0.561663,
#                0.609933, 0.6107701, 0.61300224, 0.62081474, 0.624442, 0.6010045, 0.6417411, 0.63002235, 0.63058037,
#                0.63811386, 0.60714287, 0.6155134, 0.6333705, 0.6422991, 0.6386719, 0.64397323, 0.6464844, 0.6347656,
#                0.64453125, 0.63002235, 0.63727677, 0.6501116, 0.65987724]
#
# bach128_tr = [0.35758972, 0.42092884, 0.4307946, 0.43788084, 0.4435926, 0.4481895, 0.4531071, 0.45938393, 0.4652484,
#               0.4743811, 0.4845523, 0.4956551, 0.50559723, 0.51685274, 0.5260007, 0.5356374, 0.5449228, 0.55474275,
#               0.5649903, 0.57514626, 0.5880664, 0.6005437, 0.6152049, 0.6328899, 0.65088046, 0.67329985, 0.6908169,
#               0.71419847, 0.73699963, 0.75667006, 0.7815941, 0.8103972, 0.8318698, 0.8501504, 0.87148553, 0.8939202,
#               0.9091617, 0.9217154, 0.9329098, 0.94188976, 0.9499992, 0.9525344, 0.9514043, 0.9678523, 0.97722936,
#               0.9701278, 0.97881764, 0.9816888, 0.9731364, 0.98289526]
#
# batch128_val = [0.405971, 0.42689732, 0.437221, 0.43638393, 0.44949776, 0.44391742, 0.44252232, 0.45591518, 0.45479912,
#                 0.4606585, 0.48214287, 0.49609375, 0.5019531, 0.49079242, 0.5203683, 0.52622765, 0.5220424, 0.531529,
#                 0.54910713, 0.5555245, 0.55050224, 0.56696427, 0.55803573, 0.547433, 0.54241073, 0.563337, 0.56026787,
#                 0.5705915, 0.57310265, 0.5719866, 0.5750558, 0.5488281, 0.6012835, 0.5555245, 0.5876116, 0.594308,
#                 0.60491073, 0.59905136, 0.60491073, 0.6163505, 0.6152344, 0.624163, 0.6141183, 0.624442, 0.6177455,
#                 0.63002235, 0.63783485, 0.64285713, 0.6311384, 0.6171875]
#
# plt.plot(batch32_tr, label='batch32 - Train')
# plt.plot(batch32_val, label='batch 32 - Val')
# plt.plot(batch64_tr, label='batch64 - Train', marker='x')
# plt.plot(batch64_val, label='batch64 - Val', marker='x')
# plt.plot(bach128_tr, label='batch128 - Train', marker='.')
# plt.plot(batch128_val, label='batch128 - Val', marker='.')
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.3, 1])
# plt.legend(loc='upper left')
# plt.grid(True)
# plt.show()
#
# original_dataset_tr = [0.87074316, 0.8765916, 0.87872034, 0.8793769, 0.88017267, 0.8809685, 0.88150567, 0.88242084,
#                        0.8836742, 0.88419145, 0.8854647, 0.8866187, 0.88741446, 0.8888071, 0.8900605, 0.8917317,
#                        0.8938604, 0.8955714, 0.89889383, 0.9018383, 0.90629476, 0.91140777, 0.9175155, 0.9247175,
#                        0.93432677, 0.94339883, 0.95052123, 0.9540427, 0.95959336, 0.9629954, 0.96548223, 0.9717492,
#                        0.9751711, 0.9759271, 0.97916996, 0.98245263, 0.9825919, 0.9858348, 0.9866306, 0.98830175,
#                        0.9895551, 0.9882421, 0.9883615, 0.99114674, 0.99126613, 0.99413097, 0.9941111, 0.99377286,
#                        0.9922609, 0.99174356]
#
# original_dataset_val = [0.86602014, 0.8678161, 0.8746408, 0.87248564, 0.8746408, 0.8760776, 0.8782328, 0.8800287,
#                         0.87931037, 0.8821839, 0.8811063, 0.8829023, 0.8825431, 0.88074714, 0.87679595, 0.8771552,
#                         0.87966955, 0.87895113, 0.8753592, 0.8854167, 0.8843391, 0.88182473, 0.87212646, 0.8645833,
#                         0.8509339, 0.8207615, 0.86494255, 0.875, 0.86602014, 0.8591954, 0.86494255, 0.8573994,
#                         0.8728448, 0.8573994, 0.87033045, 0.87140805, 0.8688937, 0.860273, 0.87104887, 0.87320405,
#                         0.86494255, 0.8739224, 0.8670977, 0.85380745, 0.8760776, 0.8685345, 0.8663793, 0.8667385,
#                         0.87643677, 0.86278737]
#
# plt.plot(original_dataset_tr, label='Training Accuracy')
# plt.plot(original_dataset_val, label='Validation Accuracy')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.8, 1])
# plt.legend(loc='lower right')
# plt.grid(True)
# plt.show()
#
#
# """" MM REGUL"""
# l1l1_train = [0.59807014, 0.5986433, 0.59854865, 0.59858024, 0.59839094, 0.5985171]
#
# l1l2_val =[0.5, 0.5, 0.5, 0.5, 0.5, 0.5]
#
# l1l2_train=[0.5978809, 0.5986433, 0.5997791, 0.6684966, 0.68237895, 0.68692225, 0.6872693, 0.6916864, 0.6924121, 0.69708157, 0.70342326]
#
# l1l2_val =[0.5, 0.5, 0.5, 0.6801136, 0.6857954, 0.70113635, 0.6948864, 0.6869318, 0.66136366, 0.66420454, 0.6636364]
#
# l2l1_train= [0.5994576, 0.67673135, 0.70266604, 0.70995426, 0.7144029, 0.7174318, 0.7187253, 0.7199874, 0.7218173, 0.718536, 0.7212179, 0.7221328]
#
# l2l1_val=[0.5, 0.64204544, 0.70681816, 0.7113636, 0.71875, 0.71875, 0.7278409, 0.72159094, 0.7244318, 0.72727275, 0.72159094, 0.7244318]
#
# l2l2_train=[0.65631306, 0.70197195, 0.7067361, 0.7090708, 0.71134245, 0.7154125, 0.71822053, 0.7213756, 0.7215965, 0.72475153, 0.72472, 0.72737026, 0.7319451, 0.73241836, 0.7359205, 0.7362991]
#
# l2l2_val=[0.69204545, 0.7022727, 0.68238634, 0.69545454, 0.70454544, 0.7204546, 0.7170454, 0.725, 0.7255682, 0.7340909, 0.7409091, 0.73295456, 0.7409091, 0.7340909, 0.7335227, 0.73636365]
#
#
# plt.plot(l1l1_train, label='L1/L1 - Train')
# plt.plot(l1l1_val, label='L1/L1 - Val')
#
# plt.plot(l1l2_train, label='L1/L2 - Train', marker='x')
# plt.plot(l1l2_val, label='L1/L2 - Val', marker='x')
#
# plt.plot(l2l1_train, label='L2/L1 - Train', marker='.')
# plt.plot(l2l1_val, label='L2/L1 - Val', marker='.')
#
# plt.plot(l2l2_train, label='L2/L2 - Train', marker='^')
# plt.plot(l2l2_val, label='L2/L2 - Val', marker='^')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.45, 0.8])
# plt.legend(loc='lower right')
#
# plt.grid(True)
# plt.show()
#
# """"Drop out FD"""
# drop0101_train = [0.38841462, 0.4344491, 0.44593215, 0.46507052, 0.48922607, 0.5078155, 0.5277316, 0.546626, 0.5643004, 0.5865803, 0.6150972, 0.64388865, 0.6784293, 0.7207015, 0.759268, 0.79585207, 0.83086544, 0.8593824, 0.8834769, 0.9037896, 0.9132291, 0.9306748, 0.94069386, 0.9487457, 0.95455587, 0.96016777, 0.96449864, 0.96661836, 0.9686771, 0.97253525, 0.97596645, 0.9774304, 0.98014486, 0.9816546, 0.98284405, 0.98571104, 0.98505527, 0.9862295, 0.9868395, 0.98909646, 0.9893252, 0.9890812, 0.9911704, 0.9914297, 0.99118567]
#
# drop0101_val =[0.44164824, 0.45243362, 0.4784292, 0.4983407, 0.5168695, 0.5276549, 0.52986723, 0.5445243, 0.54341817, 0.5478429, 0.5613938, 0.56277657, 0.55835176, 0.5608407, 0.5586283, 0.58849555, 0.5984513, 0.5962389, 0.59568584, 0.60038716, 0.60315263, 0.6136615, 0.6244469, 0.6214049, 0.6352323, 0.63274336, 0.6374447, 0.6393805, 0.6404867, 0.6385509, 0.6454646, 0.647677, 0.64629424, 0.6568031, 0.6498894, 0.6529314, 0.6579093, 0.65486723, 0.6465708, 0.6612279, 0.6529314, 0.6523783, 0.6609513, 0.6529314, 0.647677]
#
# drop0303_train=[0.37954268, 0.426748, 0.44181472, 0.45971787, 0.48154023, 0.49645445, 0.51374763, 0.5326115, 0.54961497, 0.5659474, 0.5848265, 0.6073351, 0.6288982, 0.65639347, 0.6864964, 0.71852076, 0.7482577, 0.78504, 0.81646967, 0.83964926, 0.8627678, 0.87980175, 0.8984674, 0.9140221, 0.92369044, 0.93374, 0.9400991, 0.948029, 0.95274115, 0.959329, 0.9625162, 0.96605414]
#
# drop0303_val =[0.42948008, 0.45602876, 0.47123894, 0.48534292, 0.50884956, 0.52018803, 0.52350664, 0.539823, 0.5392699, 0.54756635, 0.5544801, 0.57356197, 0.5824115, 0.59568584, 0.59457964, 0.6114491, 0.59513277, 0.5865597, 0.61227876, 0.6092367, 0.6200221, 0.61725664, 0.6233407, 0.62804204, 0.6261062, 0.6382743, 0.64131635, 0.6379978, 0.63274336, 0.63606197, 0.62306416, 0.6285951]
#
# drop0505_train= [0.36824694, 0.41675943, 0.43065193, 0.4458254, 0.45749143, 0.47708732, 0.49346548, 0.5082882, 0.52173847, 0.532886, 0.54827297, 0.5633092, 0.57776594, 0.59329015, 0.60648113, 0.6256805, 0.64440715, 0.6642013, 0.68346167, 0.7065345, 0.7292566, 0.75367135, 0.7774762, 0.7976515, 0.81826913, 0.8432787, 0.8600381, 0.8758978, 0.88927186, 0.9000839, 0.9118109, 0.91859704, 0.9268776, 0.933801, 0.93900114, 0.94491804, 0.949066, 0.95216167, 0.9554251, 0.95748377, 0.96035075, 0.9636752, 0.96516967, 0.9681891, 0.96901256, 0.96971405, 0.9719405, 0.97317576, 0.97518873]
#
# drop0505_val=[0.4375, 0.45022124, 0.4596239, 0.47621682, 0.49695796, 0.5157633, 0.5069137, 0.5392699, 0.53125, 0.5376106, 0.571073, 0.5683075, 0.5743916, 0.58821905, 0.58600664, 0.59900445, 0.60978985, 0.6092367, 0.6189159, 0.6158739, 0.62195796, 0.63440263, 0.6261062, 0.6319137, 0.6449115, 0.6274889, 0.64408183, 0.6368916, 0.6490597, 0.6460177, 0.6493363, 0.65320796, 0.6537611, 0.65210176, 0.6498894, 0.6537611, 0.65431416, 0.6529314, 0.65929204, 0.6634403, 0.6493363, 0.65818584, 0.66288716, 0.6645465, 0.65542036, 0.65210176, 0.66067475, 0.6512721, 0.6545907]
#
#
# plt.plot(drop0101_train, label='10% - Train')
# plt.plot(drop0101_val, label='10% - Val')
#
# plt.plot(drop0303_train, label='30% - Train', marker='x')
# plt.plot(drop0303_val, label='30% - Val', marker='x')
#
# plt.plot(drop0505_train, label='50% - Train', marker='.')
# plt.plot(drop0505_val, label='50% - Val', marker='.')
#
#
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.35, 1])
# plt.legend(loc='lower right')
#
# plt.grid(True)
# plt.show()
#
# """"Drop out NP"""
# drop0101_train = [0.7788485, 0.80515873, 0.8250577, 0.8355374, 0.83999753, 0.8441249, 0.85088265, 0.85811865, 0.8650323, 0.8679122, 0.8734431, 0.87661403, 0.8813757, 0.88381886, 0.888674, 0.8911484, 0.8935604, 0.8950991]
#
# drop0101_val =[0.80108535, 0.8115644, 0.8205464, 0.81867516, 0.8394461, 0.8400075, 0.85011226, 0.84936374, 0.86283684, 0.8714446, 0.8604042, 0.8866018, 0.88997006, 0.87649703, 0.88529193, 0.85497755, 0.8808009, 0.87181884]
#
# drop0303_train=[0.7739105, 0.80587614, 0.81745785, 0.821658, 0.82589984, 0.83266795, 0.8376375, 0.8417649, 0.8459755, 0.8489177, 0.8540432, 0.8562993, 0.8598237, 0.8635664, 0.86467886, 0.8680681, 0.87067765, 0.87033457, 0.8721852, 0.87384856, 0.8762086]
#
# drop0303_val =[0.7863024, 0.82279193, 0.8053892, 0.81998503, 0.8299027, 0.8313997, 0.83476794, 0.84693116, 0.8516093, 0.8534805, 0.8650823, 0.8594686, 0.8448728, 0.8671407, 0.8648952, 0.8740644, 0.83925897, 0.8643338, 0.8523578, 0.8624626, 0.8476796]
#
# drop0505_train= [0.76522994, 0.79791236, 0.81101197, 0.8147755, 0.81963074, 0.82130456, 0.828894, 0.83984154, 0.8463706, 0.84961426, 0.8533986, 0.8562785, 0.8571206, 0.85940784, 0.8603643, 0.8639719, 0.86477244, 0.8637432]
#
# drop0505_val=[0.7986527, 0.8183009, 0.81493264, 0.8076347, 0.809506, 0.806512, 0.8366392, 0.85890716, 0.8641467, 0.86601794, 0.8532934, 0.8736901, 0.8740644, 0.86751497, 0.86096555, 0.87032187, 0.85722303, 0.84375]
#
#
# plt.plot(drop0101_train, label='10% - Train')
# plt.plot(drop0101_val, label='10% - Val')
#
# plt.plot(drop0303_train, label='30% - Train', marker='x')
# plt.plot(drop0303_val, label='30% - Val', marker='x')
#
# plt.plot(drop0505_train, label='50% - Train', marker='.')
# plt.plot(drop0505_val, label='50% - Val', marker='.')
#
#
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.75, 1])
# plt.legend(loc='upper left')
#
# plt.grid(True)
# plt.show()
#
# """"NO REGUL NP DROPOUT"""
# drop0101_train = [0.8143505, 0.8516832, 0.86677897, 0.87914044, 0.8889755, 0.8962323, 0.9063793, 0.9149253, 0.9201443, 0.9326929, 0.9394922, 0.94659305]
#
# drop0101_val =[0.83476794, 0.8313997, 0.8486153, 0.8506737, 0.8650823, 0.8738772, 0.87743264, 0.85778445, 0.86957335, 0.86863774, 0.87238026, 0.8678892]
#
# drop0303_train=[0.8136436, 0.8519951, 0.8643046, 0.87429565, 0.8825713, 0.8895993, 0.8975943, 0.9041752, 0.91079783, 0.9188655, 0.9241574, 0.9311022, 0.9361238, 0.9417587, 0.94403553, 0.9497016, 0.952727, 0.95616823, 0.9591625, 0.9628532, 0.9643815, 0.9663361, 0.9683946, 0.9703179, 0.9722309, 0.9713368, 0.97414386, 0.9761712]
#
# drop0303_val =[0.8192365, 0.8291542, 0.8416916, 0.8611527, 0.8624626, 0.8778069, 0.8785554, 0.8830464, 0.8817365, 0.8849177, 0.8808009, 0.88416916, 0.88716316, 0.8860404, 0.88323355, 0.8847305, 0.8880988, 0.88547903, 0.88622755, 0.8766841, 0.88903445, 0.880988, 0.89015716, 0.88229793, 0.8858533, 0.8819237, 0.8785554, 0.8873503]
#
# drop0505_train= [0.81107575, 0.8504148, 0.8662799, 0.87642694, 0.8842763, 0.8918034, 0.89694965, 0.90289646, 0.9093527, 0.9139584, 0.91884476, 0.9231593, 0.9261639, 0.92978185, 0.9333687, 0.934294]
#
# drop0505_val=[0.8166168, 0.8313997, 0.83345807, 0.87238026, 0.8763099, 0.8830464, 0.8914671, 0.8916542, 0.89277697, 0.89015716, 0.8963323, 0.88828593, 0.89427394, 0.8875374, 0.8821108, 0.8731288]
#
#
# plt.plot(drop0101_train, label='10% - Train')
# plt.plot(drop0101_val, label='10% - Val')
#
# plt.plot(drop0303_train, label='30% - Train', marker='x')
# plt.plot(drop0303_val, label='30% - Val', marker='x')
#
# plt.plot(drop0505_train, label='50% - Train', marker='.')
# plt.plot(drop0505_val, label='50% - Val', marker='.')
#
#
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.75, 1])
# plt.legend(loc='upper left')
#
# plt.grid(True)
# plt.show()
#
#
# conv_3 = [0.785845, 0.79616576, 0.80464935, 0.81687564, 0.83480966, 0.8452166, 0.8493544, 0.8515273, 0.85348177, 0.85485417, 0.85567546, 0.85653836, 0.85780674, 0.85869044, 0.85933506, 0.8596677, 0.86044747, 0.86173666, 0.863057, 0.8647412]
# conv_3_val = [0.7930389, 0.7941617, 0.806512, 0.79528445, 0.8037051, 0.82653445, 0.82877994, 0.8175524, 0.85310626, 0.85797155, 0.8488024, 0.8450599, 0.84618264, 0.85516465, 0.8611527, 0.849738, 0.84113026, 0.83832335, 0.8474925, 0.85591316]
#
#
# conv_5 =[0.80480707, 0.8480444, 0.8598133, 0.8731728, 0.88506645, 0.8929262, 0.89937204, 0.90427923, 0.908521, 0.91131765, 0.91407275, 0.91760755, 0.92005074, 0.922234, 0.9241989, 0.9253322, 0.9276714]
# conv_5_val = [0.82447606, 0.8532934, 0.8663922, 0.875, 0.88416916, 0.8937126, 0.9040045, 0.90625, 0.90568864, 0.900262, 0.91242516, 0.91523206, 0.90662426, 0.91523206, 0.9075599, 0.90774703, 0.8931512]
#
# conv_6 = [0.8149015, 0.8606242, 0.8818747, 0.89393467, 0.9025014, 0.908469, 0.9149876, 0.9194477, 0.9245628, 0.9273387, 0.9312582, 0.9350841, 0.9383486, 0.9401368, 0.942684, 0.9466035, 0.9497536, 0.9532572, 0.9557836]
# conv_6_val = [0.8459955, 0.8684506, 0.8768712, 0.88622755, 0.8963323, 0.9075599, 0.9073728, 0.91654193, 0.9101796, 0.91841316, 0.92234284, 0.91822606, 0.9219686, 0.9230913, 0.9169162, 0.88622755, 0.90662426, 0.8867889, 0.9021332]
#
#
# conv_4 = [0.7865935, 0.8201401, 0.84640175, 0.8569958, 0.8629946, 0.8688167, 0.87364066, 0.8768012, 0.88025284, 0.8851288, 0.88860124, 0.8929158, 0.8967521, 0.89935124, 0.9034787, 0.90647286, 0.9090928, 0.9117127, 0.91345936, 0.9164639, 0.91791946, 0.920425, 0.92272264, 0.92328405, 0.9258312, 0.92749465, 0.92812884, 0.9292932, 0.9303745, 0.9321419, 0.93224585, 0.93281764, 0.93549997, 0.93557274]
# conv_4_val =[0.7883608, 0.83345807, 0.85591316, 0.84337574, 0.85591316, 0.8574102, 0.8596557, 0.8669536, 0.8766841, 0.8880988, 0.8924027, 0.89109284, 0.88716316, 0.89502245, 0.900262, 0.8952096, 0.8909057, 0.8808009, 0.9043788, 0.9083084, 0.9133608, 0.9171033, 0.91148955, 0.91242516, 0.912238, 0.91261226, 0.9206587, 0.9247754, 0.9249626, 0.9084955, 0.91186374, 0.9133608, 0.9148578, 0.91766465]
#
# plt.plot(conv_3, label='3 tr')
# plt.plot(conv_3_val, label='3 Val')
#
#
# plt.plot(conv_4, label='4- Train', marker='^')
# plt.plot(conv_4_val, label='4 - Val', marker='^')
#
# plt.plot(conv_5, label='5 Train', marker='x')
# plt.plot(conv_5_val, label='5 Val', marker='x')
#
# plt.plot(conv_6, label='6 - Train', marker='.')
# plt.plot(conv_6_val, label='6 - Val', marker='.')
#
#
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.75, 1])
# plt.legend(loc='upper left')
#
# plt.grid(True)
# plt.show()
#
# conv_2_tr=[0.31036586, 0.36747235, 0.3958521, 0.3985665, 0.39919177, 0.40289745, 0.40234846, 0.40605414, 0.4034617, 0.41020206, 0.40992758, 0.412337]
# conv_2_val=[0.3448562, 0.41980088, 0.40486726, 0.40376106, 0.4283739, 0.4283739, 0.43086284, 0.41509956, 0.4159292, 0.40625, 0.366427, 0.36780974]
#
# conv_4_tr=[0.36727133, 0.40954632, 0.4183454, 0.4238963, 0.42841023, 0.4314754, 0.43231416, 0.43434235, 0.4353946, 0.4372398, 0.43861228, 0.4423942, 0.4455814, 0.4501258, 0.45463973, 0.4585284, 0.46500954, 0.47155166, 0.4774533, 0.4820587, 0.4873656, 0.4913458, 0.49593595, 0.5018986, 0.50555855, 0.51086545, 0.5176668, 0.52448344, 0.5289363, 0.5336485, 0.5373542, 0.54263055, 0.54963017, 0.55437285, 0.55902404, 0.5649257, 0.5701411, 0.5756767, 0.5804956, 0.5836676, 0.5883645, 0.59086543]
# conv_4_val=[0.41178098, 0.43086284, 0.4186947, 0.42090708, 0.42948008, 0.43390486, 0.43943584, 0.44054204, 0.4256084, 0.41261062, 0.4159292, 0.43888274, 0.446073, 0.44330752, 0.42311946, 0.45436946, 0.4579646, 0.44386062, 0.48561946, 0.47759956, 0.49668142, 0.50027657, 0.49004424, 0.5058075, 0.49917036, 0.5124447, 0.5058075, 0.5154867, 0.5301438, 0.53539824, 0.49115044, 0.5442478, 0.5262721, 0.5373341, 0.5425885, 0.545354, 0.56443584, 0.5577987, 0.55835176, 0.5420354, 0.5470133, 0.5334624]
# conv_6_tr=[0.3764939, 0.41155928, 0.42252383, 0.4311704, 0.43885627, 0.44728938, 0.45489898, 0.4634388, 0.47319862, 0.48369044, 0.49608845, 0.5070682, 0.5175143, 0.52771634, 0.53857416, 0.546626, 0.55415934, 0.5664964, 0.57747614, 0.5862295, 0.5972093, 0.60852456, 0.61744565, 0.62900496, 0.63605034, 0.64353794, 0.64422417, 0.6459169, 0.6584979, 0.66624475, 0.6705604, 0.6777583, 0.68314147, 0.6878384]
# conv_6_val=[0.4153761, 0.44745576, 0.44911504, 0.42643806, 0.44966814, 0.46487832, 0.47372788, 0.48368362, 0.48672566, 0.48561946, 0.4983407, 0.51603985, 0.5318031, 0.5176991, 0.5204646, 0.53678095, 0.53097343, 0.49419248, 0.5340155, 0.54120576, 0.52240044, 0.54341817, 0.52461284, 0.51742256, 0.5442478, 0.5384403, 0.52129424, 0.5448009, 0.5542035, 0.5425885, 0.51908183, 0.5425885, 0.53567475, 0.53152657]
# conv_8_tr=[0.38234755, 0.42604652, 0.44146398, 0.45259628, 0.47309187, 0.4889516, 0.50224936, 0.5144491, 0.5278231, 0.5332062, 0.5479527, 0.5587343, 0.5724895, 0.5866565, 0.599817, 0.61578345]
# conv_8_val=[0.43556416, 0.46045354, 0.47234514, 0.45658186, 0.49557522, 0.5049779, 0.5157633, 0.5262721, 0.53263277, 0.54286504, 0.5456305, 0.5306969, 0.54120576, 0.5348451, 0.5210177, 0.5340155]
# conv_10_tr=[0.39230183, 0.4345406, 0.45160502, 0.47362563, 0.48924133, 0.5062905, 0.5179718, 0.52983606, 0.5432253, 0.5605642, 0.5742737, 0.58830345, 0.60274494, 0.6168662, 0.62876093, 0.64128095, 0.65553945, 0.66511625]
# conv_10_val=[0.44358408, 0.46487832, 0.46875, 0.48257744, 0.51908183, 0.5143805, 0.51631635, 0.5323562, 0.52986723, 0.5431416, 0.5359513, 0.5370575, 0.55365044, 0.5492257, 0.51106197, 0.54867256, 0.5448009, 0.54646015]
#
# plt.plot(conv_2_tr, label='2 Conv - tr')
# plt.plot(conv_2_val, label='2 Conv - val')
#
#
# plt.plot(conv_4_tr, label='4 Conv - tr', marker='^')
# plt.plot(conv_4_val, label='4 Conv - val', marker='^')
#
#
# plt.plot(conv_6_tr, label='6 Conv - tr', marker='+')
# plt.plot(conv_6_val, label='6 Conv - val', marker='+')
#
# plt.plot(conv_8_tr, label='8 Conv - tr', marker='.')
# plt.plot(conv_8_val, label='8 Conv - val', marker='.')
#
# plt.plot(conv_10_tr, label='10 Conv - tr', marker='x')
# plt.plot(conv_10_val, label='10 Conv - val', marker='x')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.3, 0.7])
# plt.legend(loc='lower right')
#
# plt.grid(True)
# plt.show()
#
# conv_2_tr =[0.78822565, 0.8096189, 0.81233233, 0.8137463, 0.8137359, 0.8154097, 0.81686527]
# conv_2_val =[0.8104416, 0.8134356, 0.8001497, 0.7848054, 0.77863026, 0.7969686, 0.806512]
# conv_4_tr=[0.7865935, 0.8201401, 0.84640175, 0.8569958, 0.8629946, 0.8688167, 0.87364066, 0.8768012, 0.88025284, 0.8851288, 0.88860124, 0.8929158, 0.8967521, 0.89935124, 0.9034787, 0.90647286, 0.9090928, 0.9117127, 0.91345936, 0.9164639, 0.91791946, 0.920425, 0.92272264, 0.92328405, 0.9258312, 0.92749465, 0.92812884, 0.9292932, 0.9303745, 0.9321419, 0.93224585, 0.93281764, 0.93549997, 0.93557274]
# conv_4_val=[0.7883608, 0.83345807, 0.85591316, 0.84337574, 0.85591316, 0.8574102, 0.8596557, 0.8669536, 0.8766841, 0.8880988, 0.8924027, 0.89109284, 0.88716316, 0.89502245, 0.900262, 0.8952096, 0.8909057, 0.8808009, 0.9043788, 0.9083084, 0.9133608, 0.9171033, 0.91148955, 0.91242516, 0.912238, 0.91261226, 0.9206587, 0.9247754, 0.9249626, 0.9084955, 0.91186374, 0.9133608, 0.9148578, 0.91766465]
# conv_6_tr=[0.8149015, 0.8606242, 0.8818747, 0.89393467, 0.9025014, 0.908469, 0.9149876, 0.9194477, 0.9245628, 0.9273387, 0.9312582, 0.9350841, 0.9383486, 0.9401368, 0.942684, 0.9466035, 0.9497536, 0.9532572, 0.9557836]
# conv_6_val=[0.8459955, 0.8684506, 0.8768712, 0.88622755, 0.8963323, 0.9075599, 0.9073728, 0.91654193, 0.9101796, 0.91841316, 0.92234284, 0.91822606, 0.9219686, 0.9230913, 0.9169162, 0.88622755, 0.90662426, 0.8867889, 0.9021332]
# conv_8_tr=[0.80911094, 0.85429275, 0.87183166, 0.88633484, 0.896617, 0.9042272, 0.90995574, 0.9155802, 0.9192398, 0.9225563, 0.9273387, 0.92928284, 0.9330256, 0.9375273, 0.940168]
# conv_8_val=[0.8330838, 0.8517964, 0.87350297, 0.8875374, 0.8963323, 0.9023204, 0.9150449, 0.9186003, 0.9150449, 0.9208458, 0.9189746, 0.92047155, 0.9186003, 0.9075599, 0.9206587]
# conv_10_tr=[0.809849, 0.85155845, 0.86648786, 0.8737654, 0.8810014, 0.8860853, 0.8918658, 0.8998815, 0.9051733, 0.90895766, 0.91527873, 0.91824174, 0.92180777, 0.9282224, 0.9291789, 0.93351424, 0.93680996, 0.94063586]
# conv_10_val=[0.83813626, 0.86601794, 0.8757485, 0.8710704, 0.87743264, 0.87556136, 0.90063626, 0.8851048, 0.9084955, 0.90943116, 0.91392213, 0.90007484, 0.9172904, 0.9025075, 0.91354793, 0.9069985, 0.9120509, 0.88622755]
#
#
# plt.plot(conv_2_tr, label='2 Conv - tr')
# plt.plot(conv_2_val, label='2 Conv - val')
#
# plt.plot(conv_4_tr, label='4 Conv - tr', marker='^')
# plt.plot(conv_4_val, label='4 Conv - val', marker='^')
#
# plt.plot(conv_6_tr, label='6 Conv - tr', marker='+')
# plt.plot(conv_6_val, label='6 Conv - val', marker='+')
#
# plt.plot(conv_8_tr, label='8 Conv - tr', marker='.')
# plt.plot(conv_8_val, label='8 Conv - val', marker='.')
#
# plt.plot(conv_10_tr, label='10 Conv - tr', marker='x')
# plt.plot(conv_10_val, label='10 Conv - val', marker='x')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.7, 1])
# plt.legend(loc='lower right')
#
# plt.grid(True)
# plt.show()
#
# """convolution test for NP"""
# conv_2_tr_acc =[0.78822565, 0.8096189, 0.81233233, 0.8137463, 0.8137359, 0.8154097, 0.81686527]
# conv_2_val_acc =[0.8104416, 0.8134356, 0.8001497, 0.7848054, 0.77863026, 0.7969686, 0.806512]
# conv_4_tr_acc=[0.7865935, 0.8201401, 0.84640175, 0.8569958, 0.8629946, 0.8688167, 0.87364066, 0.8768012, 0.88025284, 0.8851288, 0.88860124, 0.8929158, 0.8967521, 0.89935124, 0.9034787, 0.90647286, 0.9090928, 0.9117127, 0.91345936, 0.9164639, 0.91791946, 0.920425, 0.92272264, 0.92328405, 0.9258312, 0.92749465, 0.92812884, 0.9292932, 0.9303745, 0.9321419, 0.93224585, 0.93281764, 0.93549997, 0.93557274]
# conv_4_val_acc=[0.7883608, 0.83345807, 0.85591316, 0.84337574, 0.85591316, 0.8574102, 0.8596557, 0.8669536, 0.8766841, 0.8880988, 0.8924027, 0.89109284, 0.88716316, 0.89502245, 0.900262, 0.8952096, 0.8909057, 0.8808009, 0.9043788, 0.9083084, 0.9133608, 0.9171033, 0.91148955, 0.91242516, 0.912238, 0.91261226, 0.9206587, 0.9247754, 0.9249626, 0.9084955, 0.91186374, 0.9133608, 0.9148578, 0.91766465]
# conv_6_tr_acc=[0.8149015, 0.8606242, 0.8818747, 0.89393467, 0.9025014, 0.908469, 0.9149876, 0.9194477, 0.9245628, 0.9273387, 0.9312582, 0.9350841, 0.9383486, 0.9401368, 0.942684, 0.9466035, 0.9497536, 0.9532572, 0.9557836]
# conv_6_val_acc=[0.8459955, 0.8684506, 0.8768712, 0.88622755, 0.8963323, 0.9075599, 0.9073728, 0.91654193, 0.9101796, 0.91841316, 0.92234284, 0.91822606, 0.9219686, 0.9230913, 0.9169162, 0.88622755, 0.90662426, 0.8867889, 0.9021332]
# conv_8_tr_acc=[0.80911094, 0.85429275, 0.87183166, 0.88633484, 0.896617, 0.9042272, 0.90995574, 0.9155802, 0.9192398, 0.9225563, 0.9273387, 0.92928284, 0.9330256, 0.9375273, 0.940168]
# conv_8_val_acc=[0.8330838, 0.8517964, 0.87350297, 0.8875374, 0.8963323, 0.9023204, 0.9150449, 0.9186003, 0.9150449, 0.9208458, 0.9189746, 0.92047155, 0.9186003, 0.9075599, 0.9206587]
# conv_10_tr_acc=[0.809849, 0.85155845, 0.86648786, 0.8737654, 0.8810014, 0.8860853, 0.8918658, 0.8998815, 0.9051733, 0.90895766, 0.91527873, 0.91824174, 0.92180777, 0.9282224, 0.9291789, 0.93351424, 0.93680996, 0.94063586]
# conv_10_val_acc=[0.83813626, 0.86601794, 0.8757485, 0.8710704, 0.87743264, 0.87556136, 0.90063626, 0.8851048, 0.9084955, 0.90943116, 0.91392213, 0.90007484, 0.9172904, 0.9025075, 0.91354793, 0.9069985, 0.9120509, 0.88622755]
#
#
# plt.plot(conv_2_tr_acc, label='2 Conv - tr')
# plt.plot(conv_2_val_acc, label='2 Conv - val')
#
# plt.plot(conv_4_tr_acc, label='4 Conv - tr', marker='^')
# plt.plot(conv_4_val_acc, label='4 Conv - val', marker='^')
#
# plt.plot(conv_6_tr_acc, label='6 Conv - tr', marker='+')
# plt.plot(conv_6_val_acc, label='6 Conv - val', marker='+')
#
# plt.plot(conv_8_tr_acc, label='8 Conv - tr', marker='.')
# plt.plot(conv_8_val_acc, label='8 Conv - val', marker='.')
#
# plt.plot(conv_10_tr_acc, label='10 Conv - tr', marker='x')
# plt.plot(conv_10_val_acc, label='10 Conv - val', marker='x')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.7, 1])
# plt.legend(loc='lower right')
# plt.grid(True)
# plt.show()
#
# conv_2_val_prec =[0.78721356, 0.7989397, 0.804903, 0.80836844, 0.8103411, 0.8120794, 0.81352043]
# #conv_3_val_prec=[0.7866662, 0.7909326, 0.7972642, 0.8049809, 0.8148149, 0.8233687, 0.83013934, 0.8354629, 0.839744, 0.8430605, 0.84571373, 0.84776455, 0.8496759, 0.85138905, 0.8531065, 0.8547048, 0.8562607, 0.85779953, 0.8592188, 0.86045825]
# #conv_5_val_prec= [0.8148372, 0.841229, 0.85346216, 0.8617987, 0.8686551, 0.8741884, 0.87882906, 0.88284457, 0.88639665, 0.8896436, 0.89233536, 0.89483285, 0.89717877, 0.8993338, 0.9014553, 0.903399, 0.9052534]
# #conv_6_val_prec=[0.8292175, 0.85305065, 0.86583775, 0.8755136, 0.88224345, 0.8876478, 0.8922598, 0.8962562, 0.8995104, 0.9024427, 0.90512717, 0.9077706, 0.9101734, 0.91234994, 0.9146002, 0.9167851, 0.918867, 0.9208852, 0.92280865]
# conv_4_val_prec = [0.7904013, 0.81501585, 0.83188516, 0.8425648, 0.8506134, 0.8564546, 0.8609532, 0.86466444, 0.8676884, 0.87028867, 0.87264985, 0.87490326, 0.87683314, 0.87850577, 0.8802434, 0.88203174, 0.8838537, 0.8855899, 0.88721645, 0.8888635, 0.89035994, 0.89181983, 0.8932179, 0.8945264, 0.895844, 0.89709854, 0.89828557, 0.89934176, 0.9003381, 0.9012892, 0.90214485, 0.9029296, 0.90375596, 0.9045605]
# conv_8_val_prec=[0.81885207, 0.84481436, 0.8574839, 0.86708766, 0.87438244, 0.88063747, 0.8856649, 0.88995326, 0.8935409, 0.8967369, 0.8998974, 0.9026439, 0.9053278, 0.9080827, 0.91054904]
# conv_10_val_prec=[0.81486595, 0.83631486, 0.84877, 0.85706496, 0.86313057, 0.86774063, 0.87164766, 0.87554747, 0.8792076, 0.8824855, 0.8856539, 0.88842446, 0.8910846, 0.8938916, 0.8962241, 0.8986283, 0.9008504, 0.903015]
#
# plt.plot(conv_2_val_prec, label='2 Conv precision')
# plt.plot(conv_4_val_prec, label='4 Conv precision', marker='^')
# # plt.plot(conv_5_val_prec, label='5 Conv precision', marker='+')
# # plt.plot(conv_6_val_prec, label='6 Conv precision', marker='.')
# plt.plot(conv_8_val_prec, label='8 Conv precision', marker='.')
# plt.plot(conv_10_val_acc, label='10 Conv precision', marker='x')
#
# plt.xlabel('Epoch')
# plt.ylabel('Precision')
# plt.ylim([0.75, 1])
# plt.legend(loc='lower right')
# plt.grid(True)
# plt.show()
#
#
# """ Dropout fd with regul"""
# drop0101_train =[0.3619055, 0.4062219, 0.4111323, 0.41485322, 0.41836065, 0.42067862, 0.42041937]
#
# drop0101_val =[0.42754424, 0.44496682, 0.4170354, 0.43832964, 0.44137168, 0.43224558, 0.44054204]
#
# drop0303_train=[0.3455183, 0.39707205, 0.4037667, 0.40501717, 0.40652686, 0.4096378, 0.41087306]
#
# drop0303_val =[0.41841814, 0.4380531, 0.40099558, 0.41288716, 0.43224558, 0.42533186, 0.43390486]
#
# drop0505_train=[0.33080792, 0.37906215, 0.38760197, 0.39219216, 0.39504385, 0.39653832, 0.39627907, 0.39676705, 0.4016317, 0.39964926, 0.399878, 0.4020282]
#
# drop0505_val=[0.38080752, 0.42477876, 0.3761062, 0.39905974, 0.41620576, 0.4256084, 0.42809734, 0.42201328, 0.42173672, 0.4261615, 0.4175885, 0.4073562]
#
#
# plt.plot(drop0101_train, label='10% - Train')
# plt.plot(drop0101_val, label='10% - Val')
#
# plt.plot(drop0303_train, label='30% - Train', marker='x')
# plt.plot(drop0303_val, label='30% - Val', marker='x')
#
# plt.plot(drop0505_train, label='50% - Train', marker='.')
# plt.plot(drop0505_val, label='50% - Val', marker='.')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.30, 0.5])
# plt.legend(loc='lower right')
# plt.grid(True)
# plt.show()
#
#
# conv_2_tr =[0.3172256, 0.38209683, 0.40701488, 0.41444147, 0.41709492, 0.41866565, 0.41933665, 0.42084637, 0.4211666, 0.42173085]
# conv_2_val =[0.35011062, 0.39795354, 0.4283739, 0.42367256, 0.43418142, 0.4283739, 0.43307522, 0.42948008, 0.428927, 0.42146018]
# conv_4_tr=[0.36727133, 0.40954632, 0.4183454, 0.4238963, 0.42841023, 0.4314754, 0.43231416, 0.43434235, 0.4353946, 0.4372398, 0.43861228, 0.4423942, 0.4455814, 0.4501258, 0.45463973, 0.4585284, 0.46500954, 0.47155166, 0.4774533, 0.4820587, 0.4873656, 0.4913458, 0.49593595, 0.5018986, 0.50555855, 0.51086545, 0.5176668, 0.52448344, 0.5289363, 0.5336485, 0.5373542, 0.54263055, 0.54963017, 0.55437285, 0.55902404, 0.5649257, 0.5701411, 0.5756767, 0.5804956, 0.5836676, 0.5883645, 0.59086543]
# conv_4_val=[0.41178098, 0.43086284, 0.4186947, 0.42090708, 0.42948008, 0.43390486, 0.43943584, 0.44054204, 0.4256084, 0.41261062, 0.4159292, 0.43888274, 0.446073, 0.44330752, 0.42311946, 0.45436946, 0.4579646, 0.44386062, 0.48561946, 0.47759956, 0.49668142, 0.50027657, 0.49004424, 0.5058075, 0.49917036, 0.5124447, 0.5058075, 0.5154867, 0.5301438, 0.53539824, 0.49115044, 0.5442478, 0.5262721, 0.5373341, 0.5425885, 0.545354, 0.56443584, 0.5577987, 0.55835176, 0.5420354, 0.5470133, 0.5334624]
# conv_6_tr=[0.3798933, 0.41584444, 0.4323599, 0.44134197, 0.4517423, 0.46406406, 0.47777355, 0.49183378, 0.5020816, 0.51327485, 0.5245139, 0.53439575, 0.5442013, 0.55501336, 0.5696531, 0.58081585, 0.59319866, 0.6050629]
# conv_6_val=[0.41924778, 0.44800884, 0.45492256, 0.45077434, 0.47206858, 0.48313054, 0.49612832, 0.491427, 0.49391592, 0.49004424, 0.52350664, 0.52184737, 0.54231197, 0.5276549, 0.50995576, 0.52571905, 0.5287611, 0.517146]
# conv_8_tr=[0.38522866, 0.42973694, 0.4485246, 0.46348456, 0.48099124, 0.49906215, 0.5135494, 0.5316813, 0.54770875, 0.56616086, 0.58394206, 0.6034464, 0.6236371, 0.6383378, 0.65227604, 0.66748, 0.680305]
# conv_8_val=[0.44413716, 0.46432522, 0.4579646, 0.47649336, 0.508573, 0.5, 0.50663716, 0.51852876, 0.51852876, 0.50304204, 0.49972346, 0.51963496, 0.508573, 0.5060841, 0.5005531, 0.50884956, 0.49474558]
# conv_10_tr=[0.39634147, 0.439939, 0.46217307, 0.48684713, 0.50317955, 0.51804805, 0.5343347, 0.55249715, 0.572947, 0.5910637, 0.60983604, 0.62783074, 0.6443309, 0.6618223, 0.6798323, 0.69198626, 0.7081357]
# conv_10_val=[0.4477323, 0.46653762, 0.4800885, 0.49640486, 0.5165929, 0.5182522, 0.5215708, 0.5188053, 0.5188053, 0.5229535, 0.53207964, 0.5425885, 0.5295907, 0.5290376, 0.53207964, 0.5340155, 0.5284845]
#
#
# plt.plot(conv_2_tr, label='2 Conv - tr')
# plt.plot(conv_2_val, label='2 Conv - val')
#
# plt.plot(conv_4_tr, label='4 Conv - tr', marker='^')
# plt.plot(conv_4_val, label='4 Conv - val', marker='^')
#
# plt.plot(conv_6_tr, label='6 Conv - tr', marker='+')
# plt.plot(conv_6_val, label='6 Conv - val', marker='+')
#
# plt.plot(conv_8_tr, label='8 Conv - tr', marker='.')
# plt.plot(conv_8_val, label='8 Conv - val', marker='.')
#
# plt.plot(conv_10_tr, label='10 Conv - tr', marker='x')
# plt.plot(conv_10_val, label='10 Conv - val', marker='x')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.3, 0.7])
# plt.legend(loc='lower right')
#
# plt.grid(True)
# plt.show()
#
# """ FD"""
# filter_3x3_tr =[0.36727133, 0.40954632, 0.4183454, 0.4238963, 0.42841023, 0.4314754, 0.43231416, 0.43434235, 0.4353946, 0.4372398, 0.43861228, 0.4423942, 0.4455814, 0.4501258, 0.45463973, 0.4585284, 0.46500954, 0.47155166, 0.4774533, 0.4820587, 0.4873656, 0.4913458, 0.49593595, 0.5018986, 0.50555855, 0.51086545, 0.5176668, 0.52448344, 0.5289363, 0.5336485, 0.5373542, 0.54263055, 0.54963017, 0.55437285, 0.55902404, 0.5649257, 0.5701411, 0.5756767, 0.5804956, 0.5836676, 0.5883645, 0.59086543]
# filter_3x3_val=[0.41178098, 0.43086284, 0.4186947, 0.42090708, 0.42948008, 0.43390486, 0.43943584, 0.44054204, 0.4256084, 0.41261062, 0.4159292, 0.43888274, 0.446073, 0.44330752, 0.42311946, 0.45436946, 0.4579646, 0.44386062, 0.48561946, 0.47759956, 0.49668142, 0.50027657, 0.49004424, 0.5058075, 0.49917036, 0.5124447, 0.5058075, 0.5154867, 0.5301438, 0.53539824, 0.49115044, 0.5442478, 0.5262721, 0.5373341, 0.5425885, 0.545354, 0.56443584, 0.5577987, 0.55835176, 0.5420354, 0.5470133, 0.5334624]
# filter_5x5_tr=[0.3686128, 0.4114678, 0.41860464, 0.42366755, 0.42572626, 0.42787647, 0.43037742, 0.43374762, 0.4353031, 0.4378498, 0.44355318, 0.45114753, 0.45833015, 0.46755624, 0.47492185, 0.4803355, 0.48459017, 0.48968357, 0.49538696, 0.5022036, 0.506199, 0.5136714, 0.519329, 0.52396494, 0.5273351, 0.53471595, 0.53814715, 0.5419901]
# filter_5x5_val=[0.4159292, 0.43777654, 0.45049778, 0.44109514, 0.4499447, 0.4568584, 0.45934734, 0.4579646, 0.4488385, 0.43196902, 0.4261615, 0.46875, 0.4800885, 0.48368362, 0.45768806, 0.48340708, 0.48340708, 0.46902654, 0.5118916, 0.514104, 0.53318584, 0.5370575, 0.5420354, 0.52571905, 0.5262721, 0.52350664, 0.5318031, 0.53899336]
# filter_7x7_tr=[0.37259147, 0.41219977, 0.42002288, 0.4256195, 0.4307739, 0.43191764, 0.43592831, 0.44115898, 0.44646588, 0.45154405, 0.45953488, 0.46776974, 0.4717499, 0.4764468, 0.48186046, 0.48640487, 0.49252, 0.49654594, 0.50351506, 0.50888294, 0.5153336, 0.521159, 0.52896684, 0.53648496, 0.5409836, 0.5465345, 0.5521159, 0.5583988, 0.5642089, 0.57041556, 0.57508194, 0.5818071, 0.58729696, 0.59382385, 0.59943575, 0.6052459, 0.61134577, 0.61735415, 0.62688524, 0.6325429, 0.63989323, 0.6455966, 0.65149826, 0.65741515, 0.6653908]
# filter_7x7_val=[0.42256638, 0.43224558, 0.43611726, 0.4278208, 0.44330752, 0.45409292, 0.4574115, 0.46543142, 0.45326328, 0.43528762, 0.39103982, 0.47123894, 0.4806416, 0.5005531, 0.46128318, 0.4886615, 0.44303098, 0.4466261, 0.5096792, 0.48644912, 0.5179757, 0.5334624, 0.53207964, 0.5370575, 0.52129424, 0.5370575, 0.54397124, 0.5500553, 0.5370575, 0.54867256, 0.51963496, 0.5533739, 0.53650445, 0.5420354, 0.54397124, 0.5542035, 0.5555863, 0.548396, 0.5500553, 0.5652655, 0.5555863, 0.5290376, 0.5542035, 0.5597345, 0.5511615]
# filter_9x9_tr=[0.37280488, 0.41459396, 0.4249943, 0.43051466, 0.4380023, 0.44652689, 0.45463973, 0.46593976, 0.479268, 0.49183378, 0.5044758, 0.5165993, 0.52574915, 0.53848267, 0.5478307, 0.5606557, 0.57203203, 0.5811361, 0.5922379, 0.6056424, 0.6168509, 0.63025546, 0.64187574, 0.6569424]
# filter_9x9_val=[0.43086284, 0.44579646, 0.45492256, 0.43888274, 0.4596239, 0.46847346, 0.46487832, 0.48230088, 0.48147124, 0.49198008, 0.48091814, 0.5229535, 0.5370575, 0.5445243, 0.52461284, 0.53207964, 0.5533739, 0.53042036, 0.5550332, 0.548396, 0.55199116, 0.55088496, 0.5403761, 0.55365044]
#
# plt.plot(filter_3x3_tr, label='3x3 - Train')
# plt.plot(filter_3x3_val, label='3x3 - Val')
#
# plt.plot(filter_5x5_tr, label='5x5 - Train', marker='^')
# plt.plot(filter_5x5_val, label='5x5 - Val', marker='^')
#
# plt.plot(filter_7x7_tr, label='7x7 - Train', marker='+')
# plt.plot(filter_7x7_val, label='7x7 - Val', marker='+')
#
# plt.plot(filter_9x9_tr, label='9x9 - Train', marker='.')
# plt.plot(filter_9x9_val, label='9x9 - Val', marker='.')
#
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.3, 0.7])
# plt.legend(loc='lower right')
#
# plt.grid(True)
# plt.show()
# """ NP"""
# filter_3x3_tr =[0.7865935, 0.8201401, 0.84640175, 0.8569958, 0.8629946, 0.8688167, 0.87364066, 0.8768012, 0.88025284, 0.8851288, 0.88860124, 0.8929158, 0.8967521, 0.89935124, 0.9034787, 0.90647286, 0.9090928, 0.9117127, 0.91345936, 0.9164639, 0.91791946, 0.920425, 0.92272264, 0.92328405, 0.9258312, 0.92749465, 0.92812884, 0.9292932, 0.9303745, 0.9321419, 0.93224585, 0.93281764, 0.93549997, 0.93557274]
# filter_3x3_val=[0.7883608, 0.83345807, 0.85591316, 0.84337574, 0.85591316, 0.8574102, 0.8596557, 0.8669536, 0.8766841, 0.8880988, 0.8924027, 0.89109284, 0.88716316, 0.89502245, 0.900262, 0.8952096, 0.8909057, 0.8808009, 0.9043788, 0.9083084, 0.9133608, 0.9171033, 0.91148955, 0.91242516, 0.912238, 0.91261226, 0.9206587, 0.9247754, 0.9249626, 0.9084955, 0.91186374, 0.9133608, 0.9148578, 0.91766465]
# filter_5x5_tr=[0.81386185, 0.8590127, 0.87805915, 0.89306134, 0.9010979, 0.9084274, 0.91396874, 0.92088246, 0.92518663, 0.93007296, 0.9340652, 0.9380679, 0.9406254, 0.94390035, 0.94853723, 0.9513547]
# filter_5x5_val=[0.8291542, 0.86096555, 0.87967813, 0.8925898, 0.8965195, 0.8961452, 0.91541916, 0.9163548, 0.91560626, 0.9245883, 0.9257111, 0.91747755, 0.9120509, 0.9127994, 0.9189746, 0.9071856]
# filter_7x7_tr=[0.8104624, 0.8537001, 0.8779448, 0.8917826, 0.8986027, 0.90022457, 0.90952945, 0.91403115, 0.91843927, 0.92143345, 0.9248747, 0.9301042, 0.934086, 0.93820304, 0.94178987, 0.94363004, 0.9473936, 0.94901544]
# filter_7x7_val=[0.83158684, 0.8594686, 0.86732787, 0.875, 0.8873503, 0.87443864, 0.9099925, 0.90662426, 0.9051272, 0.903256, 0.9098054, 0.91541916, 0.9159805, 0.9049401, 0.9075599, 0.9041916, 0.8905314, 0.8924027]
# filter_9x9_tr=[0.8053268, 0.853565, 0.8760942, 0.88356936, 0.89371634, 0.8995176, 0.9045391, 0.9093735, 0.913948, 0.9175452, 0.9209656, 0.9260079, 0.92930365, 0.93201715, 0.9375793, 0.9389828, 0.9419146]
# filter_9x9_val=[0.82672155, 0.8575973, 0.88061374, 0.8894087, 0.8845434, 0.8914671, 0.9040045, 0.9099925, 0.90475297, 0.9084955, 0.9131737, 0.91523206, 0.89296407, 0.8886602, 0.9051272, 0.8858533, 0.88547903]
#
# plt.plot(filter_3x3_tr, label='3x3 - Train')
# plt.plot(filter_3x3_val, label='3x3 - Val')
#
# plt.plot(filter_5x5_tr, label='5x5 - Train', marker='^')
# plt.plot(filter_5x5_val, label='5x5 - Val', marker='^')
#
# plt.plot(filter_7x7_tr, label='7x7 - Train', marker='+')
# plt.plot(filter_7x7_val, label='7x7 - Val', marker='+')
#
# plt.plot(filter_9x9_tr, label='9x9 - Train', marker='.')
# plt.plot(filter_9x9_val, label='9x9 - Val', marker='.')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.75, 1])
# plt.legend(loc='lower right')
#
# plt.grid(True)
# plt.show()
#
# """ MAX  POOL FOR FD"""
#
# pool_4_tr=[0.36727133, 0.40954632, 0.4183454, 0.4238963, 0.42841023, 0.4314754, 0.43231416, 0.43434235, 0.4353946, 0.4372398, 0.43861228, 0.4423942, 0.4455814, 0.4501258, 0.45463973, 0.4585284, 0.46500954, 0.47155166, 0.4774533, 0.4820587, 0.4873656, 0.4913458, 0.49593595, 0.5018986, 0.50555855, 0.51086545, 0.5176668, 0.52448344, 0.5289363, 0.5336485, 0.5373542, 0.54263055, 0.54963017, 0.55437285, 0.55902404, 0.5649257, 0.5701411, 0.5756767, 0.5804956, 0.5836676, 0.5883645, 0.59086543]
# pool_4_val=[0.41178098, 0.43086284, 0.4186947, 0.42090708, 0.42948008, 0.43390486, 0.43943584, 0.44054204, 0.4256084, 0.41261062, 0.4159292, 0.43888274, 0.446073, 0.44330752, 0.42311946, 0.45436946, 0.4579646, 0.44386062, 0.48561946, 0.47759956, 0.49668142, 0.50027657, 0.49004424, 0.5058075, 0.49917036, 0.5124447, 0.5058075, 0.5154867, 0.5301438, 0.53539824, 0.49115044, 0.5442478, 0.5262721, 0.5373341, 0.5425885, 0.545354, 0.56443584, 0.5577987, 0.55835176, 0.5420354, 0.5470133, 0.5334624]
# pool_5_tr=[0.3595884, 0.40712163, 0.4149447, 0.42102936, 0.42281356, 0.4264125, 0.42891344, 0.43017918, 0.43234465, 0.4345101, 0.43937477, 0.4500038, 0.46507052, 0.47617233, 0.48158598, 0.4860389, 0.4907968, 0.49360275, 0.49890965, 0.5006176, 0.5047503, 0.51008767, 0.5125886, 0.5170263, 0.5228974, 0.5279909, 0.53274876, 0.5386199, 0.54418606, 0.5488677, 0.5540831, 0.55862755, 0.56385815, 0.5674266, 0.57308424, 0.57590544, 0.5800534, 0.5838963, 0.5877545, 0.5896912, 0.59364086, 0.59537935]
# pool_5_val=[0.42173672, 0.43335176, 0.43832964, 0.44109514, 0.44054204, 0.43888274, 0.4482854, 0.44303098, 0.43362832, 0.44551992, 0.44800884, 0.47538716, 0.49391592, 0.49723452, 0.4875553, 0.49308628, 0.5096792, 0.48340708, 0.5165929, 0.5096792, 0.5370575, 0.5348451, 0.539823, 0.5287611, 0.5306969, 0.5348451, 0.5492257, 0.5492257, 0.55143803, 0.54646015, 0.54507744, 0.56332964, 0.56222343, 0.5647124, 0.5743916, 0.58158183, 0.58932525, 0.58213496, 0.5782633, 0.57134956, 0.57632744, 0.55696905]
# pool_6_tr=[0.3188872, 0.37836066, 0.3886237, 0.3900267, 0.39220738, 0.39377812, 0.39522684, 0.39713305, 0.39906976, 0.40102172, 0.4026992, 0.40446818, 0.40635914]
# pool_6_val=[0.35896018, 0.3949115, 0.39823008, 0.3846792, 0.40763274, 0.40403762, 0.40873894, 0.4164823, 0.4153761, 0.41620576, 0.36559734, 0.33683628, 0.40763274]
# plt.plot(pool_4_tr, label='4 MaxP - Train')
# plt.plot(pool_4_val, label='4 MaxP - Val')
#
# plt.plot(pool_5_tr, label='5 MaxP - Train', marker='^')
# plt.plot(pool_5_val, label='5 MaxP - Val', marker='^')
#
# plt.plot(pool_6_tr, label='6 MaxP - Train', marker='+')
# plt.plot(pool_6_val, label='6 MaxP - Val', marker='+')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.3, 0.6])
# plt.legend(loc='upper left')
#
# plt.grid(True)
# plt.show()
#
# """NP MAX POOL"""
#
# pool_4_tr=[0.80911094, 0.85429275, 0.87183166, 0.88633484, 0.896617, 0.9042272, 0.90995574, 0.9155802, 0.9192398, 0.9225563, 0.9273387, 0.92928284, 0.9330256, 0.9375273, 0.940168]
# pool_4_val=[0.8330838, 0.8517964, 0.87350297, 0.8875374, 0.8963323, 0.9023204, 0.9150449, 0.9186003, 0.9150449, 0.9208458, 0.9189746, 0.92047155, 0.9186003, 0.9075599, 0.9206587]
# pool_5_tr=[0.8266072, 0.8669349, 0.88852847, 0.8994968, 0.9084586, 0.91505, 0.9204458, 0.92713076, 0.93201715, 0.93706983, 0.9417795, 0.9465619, 0.9507413, 0.953881, 0.9579357, 0.959412, 0.9622814, 0.96579546]
# pool_5_val=[0.83738774, 0.8785554, 0.88435626, 0.8867889, 0.8967066, 0.9025075, 0.9079341, 0.91373503, 0.9187874, 0.9150449, 0.9245883, 0.9266467, 0.9273952, 0.90625, 0.9187874, 0.90007484, 0.9219686, 0.9187874]
# pool_6_tr=[0.8263681, 0.86787057, 0.8857526, 0.89724076, 0.90868735, 0.9190423, 0.9252906, 0.93068635, 0.9371426, 0.9423409, 0.9474144, 0.9512923, 0.95559645, 0.95950556, 0.9619799]
# pool_6_val=[0.8495509, 0.87556136, 0.87818116, 0.8955838, 0.89502245, 0.9101796, 0.90063626, 0.92047155, 0.9193488, 0.9208458, 0.91261226, 0.9083084, 0.89782935, 0.8925898, 0.90568864]
# plt.plot(pool_4_tr, label='4 MaxP - Train')
# plt.plot(pool_4_val, label='4 MaxP - Val')
#
# plt.plot(pool_5_tr, label='5 MaxP - Train', marker='^')
# plt.plot(pool_5_val, label='5 MaxP - Val', marker='^')
#
# plt.plot(pool_6_tr, label='6 MaxP - Train', marker='+')
# plt.plot(pool_6_val, label='6 MaxP - Val', marker='+')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.8, 1])
# plt.legend(loc='lower right')
#
# plt.grid(True)
# plt.show()
#
# """NP layers """
#
# layers1_tr=[0.8178, 0.8651, 0.8875, 0.8998, 0.9112, 0.9181, 0.9252, 0.9307, 0.9359, 0.9407, 0.9451, 0.9482, 0.9518, 0.9547, 0.9582, 0.9609, 0.9632, 0.9663, 0.9680, 0.9694, 0.9713, 0.9724, 0.9755, 0.9755, 0.9773]
# layers1_val=[0.8398, 0.8786, 0.8707, 0.8943, 0.8980, 0.9126, 0.9203, 0.9222, 0.9251, 0.9257, 0.9130, 0.9231, 0.9304, 0.8900, 0.9173, 0.9175, 0.9308, 0.9160, 0.9315, 0.9429, 0.9281, 0.9343, 0.9360, 0.9330, 0.9354]
#
# layers2_tr=[0.8266072, 0.8669349, 0.88852847, 0.8994968, 0.9084586, 0.91505, 0.9204458, 0.92713076, 0.93201715, 0.93706983, 0.9417795, 0.9465619, 0.9507413, 0.953881, 0.9579357, 0.959412, 0.9622814, 0.96579546]
# layers2_val=[0.83738774, 0.8785554, 0.88435626, 0.8867889, 0.8967066, 0.9025075, 0.9079341, 0.91373503, 0.9187874, 0.9150449, 0.9245883, 0.9266467, 0.9273952, 0.90625, 0.9187874, 0.90007484, 0.9219686, 0.9187874]
#
# layers4_tr=[0.8295388, 0.87394214, 0.89364356, 0.90531886, 0.9137712, 0.92247313, 0.9312374, 0.9374649, 0.9438172, 0.9435157]
# layers4_val=[0.8568488, 0.87163174, 0.8953967, 0.8924027, 0.9079341, 0.9041916, 0.9064371, 0.8935254, 0.89427394, 0.8866018]
#
# layers8_tr=[0.8087055, 0.85744286, 0.87035537, 0.8603747, 0.8514337, 0.866571, 0.8824361, 0.887572, 0.88283116, 0.883351, 0.8954214, 0.8482212, 0.8418481, 0.8588984, 0.8707296]
# layers8_val=[0.8299027, 0.85778445, 0.8594686, 0.84636974, 0.81006736, 0.8633982, 0.8821108, 0.88248503, 0.8821108, 0.88342065, 0.8828593, 0.82765716, 0.8544162, 0.8544162, 0.8652695]
#
# plt.plot(layers1_tr, label='1 Layer - Train')
# plt.plot(layers1_val, label='1 Layer - Val')
#
# plt.plot(layers2_tr, label='2 Layers- Train', marker='^')
# plt.plot(layers2_val, label='2 Layers- Val', marker='^')
#
# plt.plot(layers4_tr, label='4 Layers - Train', marker='+')
# plt.plot(layers4_val, label='4 Layers - Val', marker='+')
#
# plt.plot(layers8_tr, label='8 Layers - Train', marker='x')
# plt.plot(layers8_val, label='8 Layers - Val', marker='x')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.8, 1])
# plt.legend(loc='lower right')
#
# plt.grid(True)
# plt.show()
#
# """ FD LAYERS"""
# layers1_tr=[0.39161587, 0.4328174, 0.45538697, 0.47836828, 0.49637818, 0.51083493, 0.5288906, 0.546199, 0.5612657, 0.5771864, 0.5915669, 0.6075944, 0.6212581, 0.6401525, 0.6560122, 0.67449486, 0.69226074, 0.7090202, 0.730934, 0.746504, 0.76492566, 0.7813038, 0.7981853]
# layers1_val=[0.4267146, 0.45464602, 0.49363938, 0.48949116, 0.51603985, 0.53152657, 0.5362279, 0.54756635, 0.54728985, 0.54728985, 0.56803095, 0.5647124, 0.57300884, 0.58932525, 0.5818584, 0.6034292, 0.60370576, 0.61393803, 0.59513277, 0.5970686, 0.6009403, 0.5962389, 0.59568584]
#
# l1layers_tr=[0.2394, 0.3357, 0.3948, 0.4038, 0.4079, 0.4110, 0.4130, 0.4161, 0.4179, 0.4207, 0.4232, 0.4249, 0.4259, 0.4274, 0.4289, 0.4294, 0.4303, 0.4318, 0.4322, 0.4331, 0.4345, 0.4346, 0.4349, 0.4363, 0.4378, 0.4384, 0.4382, 0.4385, 0.4387, 0.4393, 0.4401, 0.4405, 0.4402, 0.4409]
# l1layers_val=[0.2118, 0.4074, 0.4038, 0.3963, 0.4300, 0.4170, 0.4353, 0.4334, 0.4339, 0.4367, 0.3941, 0.3700, 0.4403, 0.4461, 0.4231, 0.4488, 0.3999, 0.4356, 0.4403, 0.4544, 0.4314, 0.4463, 0.4621, 0.4688, 0.4710, 0.4674, 0.4593, 0.4638, 0.4765, 0.4693, 0.4264, 0.4472, 0.4524, 0.4322]
#
# layers2_tr=[0.3595884, 0.40712163, 0.4149447, 0.42102936, 0.42281356, 0.4264125, 0.42891344, 0.43017918, 0.43234465, 0.4345101, 0.43937477, 0.4500038, 0.46507052, 0.47617233, 0.48158598, 0.4860389, 0.4907968, 0.49360275, 0.49890965, 0.5006176, 0.5047503, 0.51008767, 0.5125886, 0.5170263, 0.5228974, 0.5279909, 0.53274876, 0.5386199, 0.54418606, 0.5488677, 0.5540831, 0.55862755, 0.56385815, 0.5674266, 0.57308424, 0.57590544, 0.5800534, 0.5838963, 0.5877545, 0.5896912, 0.59364086, 0.59537935]
# layers2_val=[0.42173672, 0.43335176, 0.43832964, 0.44109514, 0.44054204, 0.43888274, 0.4482854, 0.44303098, 0.43362832, 0.44551992, 0.44800884, 0.47538716, 0.49391592, 0.49723452, 0.4875553, 0.49308628, 0.5096792, 0.48340708, 0.5165929, 0.5096792, 0.5370575, 0.5348451, 0.539823, 0.5287611, 0.5306969, 0.5348451, 0.5492257, 0.5492257, 0.55143803, 0.54646015, 0.54507744, 0.56332964, 0.56222343, 0.5647124, 0.5743916, 0.58158183, 0.58932525, 0.58213496, 0.5782633, 0.57134956, 0.57632744, 0.55696905]
#
# layers4_tr=[0.31038108, 0.39559284, 0.41642395, 0.42037362, 0.4242928, 0.42651924, 0.42891344, 0.4313839, 0.43394586, 0.43614182]
# layers4_val=[0.37361726, 0.43086284, 0.41731194, 0.43307522, 0.44690266, 0.42699116, 0.446073, 0.43667036, 0.43473452, 0.44220132]
#
# layers8_tr=[0.2297561, 0.23020968, 0.23016393, 0.23016393, 0.23019443, 0.23020968]
# layers8_val=[0.23561947, 0.23561947, 0.23561947, 0.23561947, 0.23561947, 0.23561947]
#
# plt.plot(layers1_tr, label='1 Layer L2 - Train')
# plt.plot(layers1_val, label='1 Layer L2 - Val')
#
# plt.plot(l1layers_tr, label='1 Layer L1 - Train', marker='o')
# plt.plot(l1layers_val, label='1 Layer L1 - Val', marker='o')
#
# plt.plot(layers2_tr, label='2 Layers - Train', marker='^')
# plt.plot(layers2_val, label='2 Layers - Val', marker='^')
#
# plt.plot(layers4_tr, label='4 Layers - Train', marker='+')
# plt.plot(layers4_val, label='4 Layers - Val', marker='+')
#
# plt.plot(layers8_tr, label='8 Layers - Train', marker='x')
# plt.plot(layers8_val, label='8 Layers - Val', marker='x')
#
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.ylim([0.2, 1])
# plt.legend(loc='upper left')
#
# plt.grid(True)
# plt.show()

"""neurons in NP"""

neurons_128_tr=[0.82174194, 0.86207974, 0.8792444, 0.88914186, 0.90056765, 0.90851057, 0.9136049, 0.91908383, 0.92271227, 0.9273699, 0.9322563, 0.9350425, 0.9394714, 0.9422785, 0.9461044]
neurons_128_val=[0.8504865, 0.87050897, 0.8738772, 0.88529193, 0.89988774, 0.90306884, 0.9064371, 0.9098054, 0.91523206, 0.91672903, 0.909244, 0.90175897, 0.9045659, 0.8622754, 0.8620883]

neurons_256_tr=[0.8178, 0.8651, 0.8875, 0.8998, 0.9112, 0.9181, 0.9252, 0.9307, 0.9359, 0.9407, 0.9451, 0.9482, 0.9518, 0.9547, 0.9582, 0.9609, 0.9632, 0.9663, 0.9680, 0.9694, 0.9713, 0.9724, 0.9755, 0.9755, 0.9773]
neurons_256_val=[0.8398, 0.8786, 0.8707, 0.8943, 0.8980, 0.9126, 0.9203, 0.9222, 0.9251, 0.9257, 0.9130, 0.9231, 0.9304, 0.8900, 0.9173, 0.9175, 0.9308, 0.9160, 0.9315, 0.9429, 0.9281, 0.9343, 0.9360, 0.9330, 0.9354]

neurons_512_tr=[0.82957006, 0.87700915, 0.8978438, 0.90991414, 0.91751397, 0.9249579, 0.93107104, 0.9366748, 0.9410101, 0.9452623, 0.9485996, 0.9536627, 0.95713514, 0.95890254]
neurons_512_val=[0.85872006, 0.8860404, 0.8993263, 0.909244, 0.91242516, 0.9161677, 0.92140716, 0.9217814, 0.9230913, 0.92028445, 0.9200973, 0.91354793, 0.9187874, 0.8931512]

plt.plot(neurons_128_tr, label='128 Neurons - Train')
plt.plot(neurons_128_val, label='128 Neurons - Val')

plt.plot(neurons_256_tr, label='256 Neurons - Train', marker='x')
plt.plot(neurons_256_val, label='256 Neurons - Val', marker='x')

plt.plot(neurons_512_tr, label='512 Neurons - Train', marker='^')
plt.plot(neurons_512_val, label='512 Neurons - Val', marker='^')

plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.8, 1])
plt.legend(loc='upper left')

plt.grid(True)
plt.show()

neurons_256_tr=[0.36207318, 0.41155928, 0.4184674, 0.42127335, 0.42324057, 0.42479604, 0.42752573, 0.4285627, 0.43133816, 0.43342736, 0.43661457, 0.4411285]
neurons_256_val=[0.41454646, 0.4358407, 0.43832964, 0.4386062, 0.44386062, 0.44579646, 0.45271018, 0.44441372, 0.43943584, 0.43445796, 0.44220132, 0.44330752]

neurons_512_tr=[0.3595884, 0.40712163, 0.4149447, 0.42102936, 0.42281356, 0.4264125, 0.42891344, 0.43017918, 0.43234465, 0.4345101, 0.43937477, 0.4500038, 0.46507052, 0.47617233, 0.48158598, 0.4860389, 0.4907968, 0.49360275, 0.49890965, 0.5006176, 0.5047503, 0.51008767, 0.5125886, 0.5170263, 0.5228974, 0.5279909, 0.53274876, 0.5386199, 0.54418606, 0.5488677, 0.5540831, 0.55862755, 0.56385815, 0.5674266, 0.57308424, 0.57590544, 0.5800534, 0.5838963, 0.5877545, 0.5896912, 0.59364086, 0.59537935]
neurons_512_val=[0.42173672, 0.43335176, 0.43832964, 0.44109514, 0.44054204, 0.43888274, 0.4482854, 0.44303098, 0.43362832, 0.44551992, 0.44800884, 0.47538716, 0.49391592, 0.49723452, 0.4875553, 0.49308628, 0.5096792, 0.48340708, 0.5165929, 0.5096792, 0.5370575, 0.5348451, 0.539823, 0.5287611, 0.5306969, 0.5348451, 0.5492257, 0.5492257, 0.55143803, 0.54646015, 0.54507744, 0.56332964, 0.56222343, 0.5647124, 0.5743916, 0.58158183, 0.58932525, 0.58213496, 0.5782633, 0.57134956, 0.57632744, 0.55696905]

neurons_1024_tr=[0.3159756, 0.3975448, 0.41093403, 0.4137705, 0.41712543, 0.41962638, 0.42151734, 0.42496377, 0.43210065, 0.44234845, 0.45229128, 0.46224934, 0.47030118, 0.47893253, 0.48591688, 0.4919558, 0.49805567, 0.5032253, 0.5091117, 0.5142509, 0.519329, 0.52317196, 0.5252154, 0.5286161, 0.5320015, 0.5351582, 0.5378574, 0.53971785, 0.5422493, 0.54319483, 0.546687, 0.54879147, 0.55075866, 0.5529699, 0.55432713]
neurons_1024_val=[0.3625553, 0.41841814, 0.4159292, 0.41344026, 0.43667036, 0.43169248, 0.43777654, 0.43390486, 0.43722346, 0.43998894, 0.43362832, 0.45022124, 0.49280974, 0.49585176, 0.48091814, 0.45353982, 0.47317478, 0.4789823, 0.5107854, 0.52682525, 0.49806416, 0.5409292, 0.5409292, 0.54286504, 0.54646015, 0.5500553, 0.5517146, 0.5589049, 0.56277657, 0.56443584, 0.49308628, 0.55586284, 0.55143803, 0.56388277, 0.5492257]

plt.plot(neurons_256_tr, label='256 Neurons - Train')
plt.plot(neurons_256_val, label='256 Neurons - Val')

plt.plot(neurons_512_tr, label='512 Neurons - Train', marker='^')
plt.plot(neurons_512_val, label='512 Neurons - Val', marker='^')

plt.plot(neurons_1024_tr, label='1024 Neurons - Train', marker='x')
plt.plot(neurons_1024_val, label='1024 Neurons - Val', marker='x')

plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.3, 0.7])
plt.legend(loc='upper left')

plt.grid(True)
plt.show()

""""NP FINAL COMP"""

original_acc_tr=[0.8220746, 0.8621109, 0.88106376, 0.89548373, 0.907315, 0.9201235, 0.9284927, 0.938411, 0.94740397, 0.9551078, 0.9602749, 0.96575385, 0.96971494, 0.9736344, 0.97794896, 0.9785, 0.98136944, 0.9828977, 0.98419726, 0.9864221, 0.98789847, 0.98883414, 0.9894787, 0.99035203, 0.99000895, 0.99116296, 0.9916932, 0.99216104, 0.9927848, 0.9938245, 0.99360615, 0.99414676, 0.9937621, 0.99446905, 0.9949369, 0.9945626, 0.9939388, 0.9960597, 0.9951552, 0.99506164, 0.9960909, 0.9964444, 0.9962053, 0.99587256, 0.9963716, 0.9962261, 0.99627805, 0.996434, 0.9966523, 0.99664193]
original_acc_val=[0.8536677, 0.8787425, 0.89127994, 0.8916542, 0.88267213, 0.8856662, 0.88398206, 0.8895958, 0.89801645, 0.8847305, 0.8982036, 0.89202845, 0.8895958, 0.88342065, 0.8935254, 0.8903443, 0.8905314, 0.89502245, 0.8909057, 0.8997006, 0.89408684, 0.8944611, 0.8918413, 0.8955838, 0.8924027, 0.89277697, 0.8903443, 0.88342065, 0.8895958, 0.88641465, 0.89202845, 0.88622755, 0.8897829, 0.88416916, 0.8856662, 0.8879117, 0.8894087, 0.8905314, 0.8905314, 0.89876497, 0.88529193, 0.89408684, 0.88622755, 0.88922155, 0.8982036, 0.8888473, 0.8938997, 0.8895958, 0.8894087, 0.8909057]
final_acc_tr=[0.8178, 0.8651, 0.8875, 0.8998, 0.9112, 0.9181, 0.9252, 0.9307, 0.9359, 0.9407, 0.9451, 0.9482, 0.9518, 0.9547, 0.9582, 0.9609, 0.9632, 0.9663, 0.9680, 0.9694, 0.9713, 0.9724, 0.9755, 0.9755, 0.9773]
final_acc_val=[0.8398, 0.8786, 0.8707, 0.8943, 0.8980, 0.9126, 0.9203, 0.9222, 0.9251, 0.9257, 0.9130, 0.9231, 0.9304, 0.8900, 0.9173, 0.9175, 0.9308, 0.9160, 0.9315, 0.9429, 0.9281, 0.9343, 0.9360, 0.9330, 0.9354]

original_recall_val=[0.7687246, 0.78863144, 0.8041797, 0.81749284, 0.82819974, 0.8376137, 0.84598684, 0.85393775, 0.8616958, 0.8689414, 0.87553513, 0.8817554, 0.8874465, 0.89268976, 0.8975121, 0.901881, 0.90584487, 0.90953803, 0.91287196, 0.91601473, 0.9189648, 0.9216909, 0.924217, 0.92655927, 0.9286849, 0.93068314, 0.93255633, 0.93427783, 0.9359215, 0.9374948, 0.9389628, 0.9403563, 0.94165397, 0.9429129, 0.9440919, 0.9451889, 0.946263, 0.9473618, 0.9483788, 0.9493614, 0.9503028, 0.9512066, 0.95205635, 0.9528598, 0.9536666, 0.95442986, 0.9551712, 0.95587796, 0.95655066, 0.9571832]
final_recall_val=[0.7695, 0.7976, 0.8157, 0.8291, 0.8403, 0.8495, 0.8576, 0.8646, 0.8708, 0.8765, 0.8814, 0.8857, 0.8896, 0.8932, 0.8964, 0.8996, 0.9026, 0.9055, 0.9081, 0.9108, 0.9133, 0.9157, 0.9180, 0.9202, 0.9222]

original_pres_val=[0.82647294, 0.8523369, 0.8669292, 0.8770238, 0.8853033, 0.89310724, 0.8996549, 0.9055879, 0.910968, 0.9158689, 0.92017806, 0.9240709, 0.9275564, 0.93074006, 0.9338296, 0.9364795, 0.93903345, 0.9413325, 0.94350255, 0.9455594, 0.94746983, 0.9492314, 0.95086175, 0.95239514, 0.95380336, 0.95516294, 0.9564243, 0.95762426, 0.958755, 0.95984197, 0.9608556, 0.9618189, 0.9627065, 0.9635449, 0.9643677, 0.9651364, 0.9658049, 0.9664816, 0.9670907, 0.9676671, 0.9682526, 0.96881807, 0.9693489, 0.9698433, 0.9703229, 0.97077465, 0.97119915, 0.9716135, 0.9720195, 0.9724216]
final_pre_val=[0.8171, 0.8437, 0.8605, 0.8715, 0.8802, 0.8870, 0.8927, 0.8976, 0.9019, 0.9057, 0.9093, 0.9126, 0.9157, 0.9186, 0.9213, 0.9238, 0.9261, 0.9284, 0.9304, 0.9323, 0.9340, 0.9356, 0.9373, 0.9387, 0.9402]

plt.plot(original_acc_tr, label='Org Accuracy - Train')
plt.plot(original_acc_val, label='Org Accuracy - Val')
plt.plot(final_acc_tr, label='Final Accuracy - Train', marker='o')
plt.plot(final_acc_val, label='Final Accuracy - Val', marker='o')

plt.plot(original_pres_val, label='Original Precision', marker='^')
plt.plot(final_pre_val, label='Final Precision', marker='^')

plt.plot(original_recall_val, label='Original Recall', marker='x')
plt.plot(final_recall_val, label='Final Recall', marker='x')

plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.7, 1])
plt.legend(loc='lower right')

plt.grid(True)
plt.show()

"""
final_recall_tr=
final_recall_val=
final_pres_tr=
final_pres_val=


plt.plot(final_acc_tr, label='Accuracy - Train')
plt.plot(final_acc_val, label='Accuracy - Val')

plt.plot(final_pres_tr, label='Precision - Train', marker='^')
plt.plot(final_pres_val, label='Precision - Val', marker='^')

plt.plot(final_recall_tr, label='Recall - Train', marker='x')
plt.plot(final_recall_val, label='Recall - Val', marker='x')

plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.7, 1])
plt.legend(loc='lower right')

plt.grid(True)
plt.show()
"""


data_aug_tr = [0.3946494, 0.43655357, 0.45026305, 0.47289363, 0.49744567, 0.5199085, 0.5403889, 0.5636599, 0.59060615, 0.625467, 0.66441476, 0.71112466, 0.7570263, 0.80381244, 0.83762103, 0.86879146, 0.8980557, 0.91719407, 0.93212354, 0.94372857, 0.9533664, 0.9626687, 0.9672741, 0.9724285, 0.97744566, 0.9798246, 0.9814106, 0.9814411, 0.98357606, 0.9851315, 0.987907, 0.98722076, 0.9899504, 0.9891422, 0.99327487, 0.9927259, 0.99324435, 0.9940374, 0.9907434, 0.99522686, 0.9953946, 0.99402213, 0.99306136, 0.99350363, 0.99550134, 0.9985665, 0.9980785, 0.99458635, 0.99540985, 0.9965078]

data_aug_val =[0.41399336, 0.42146018, 0.45768806, 0.48230088, 0.5011062, 0.52571905, 0.5409292, 0.5528208, 0.5608407, 0.5575221, 0.57992256, 0.59457964, 0.5829646, 0.5862832, 0.585177, 0.60896015, 0.59457964, 0.6020465, 0.6125553, 0.60868365, 0.6285951, 0.6225111, 0.6214049, 0.63384956, 0.6498894, 0.6482301, 0.6465708, 0.6308075, 0.6490597, 0.647677, 0.6368916, 0.64961284, 0.642146, 0.6670354, 0.65486723, 0.6568031, 0.6706305, 0.6670354, 0.66288716, 0.6653761, 0.6642699, 0.6609513, 0.6598451, 0.64629424, 0.6664823, 0.67809737, 0.6816925, 0.65099555, 0.6601217, 0.66150445]



final_fd_tr=[0.3595884, 0.40712163, 0.4149447, 0.42102936, 0.42281356, 0.4264125, 0.42891344, 0.43017918, 0.43234465, 0.4345101, 0.43937477, 0.4500038, 0.46507052, 0.47617233, 0.48158598, 0.4860389, 0.4907968, 0.49360275, 0.49890965, 0.5006176, 0.5047503, 0.51008767, 0.5125886, 0.5170263, 0.5228974, 0.5279909, 0.53274876, 0.5386199, 0.54418606, 0.5488677, 0.5540831, 0.55862755, 0.56385815, 0.5674266, 0.57308424, 0.57590544, 0.5800534, 0.5838963, 0.5877545, 0.5896912, 0.59364086, 0.59537935]
final_fd_val=[0.42173672, 0.43335176, 0.43832964, 0.44109514, 0.44054204, 0.43888274, 0.4482854, 0.44303098, 0.43362832, 0.44551992, 0.44800884, 0.47538716, 0.49391592, 0.49723452, 0.4875553, 0.49308628, 0.5096792, 0.48340708, 0.5165929, 0.5096792, 0.5370575, 0.5348451, 0.539823, 0.5287611, 0.5306969, 0.5348451, 0.5492257, 0.5492257, 0.55143803, 0.54646015, 0.54507744, 0.56332964, 0.56222343, 0.5647124, 0.5743916, 0.58158183, 0.58932525, 0.58213496, 0.5782633, 0.57134956, 0.57632744, 0.55696905]

plt.plot(data_aug_tr, label='Data Augmented - Train')
plt.plot(data_aug_val, label='Data Augmented - Val')
plt.plot(final_fd_tr, label='Final - Train')
plt.plot(final_fd_val, label='Final - Val')

plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.35, 1])
plt.legend(loc='lower right')

plt.grid(True)
plt.show()


